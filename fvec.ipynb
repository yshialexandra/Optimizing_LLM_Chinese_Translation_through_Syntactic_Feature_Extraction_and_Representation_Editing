{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "2024-09-17 11:07:20 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ad363735af434abfff49186c24e248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 11:07:21 INFO: Downloaded file to C:\\Users\\Administrator\\stanza_resources\\resources.json\n",
      "2024-09-17 11:07:21 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-09-17 11:07:22 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "===================================\n",
      "| Processor    | Package          |\n",
      "-----------------------------------\n",
      "| tokenize     | gsdsimp          |\n",
      "| pos          | gsdsimp_charlm   |\n",
      "| lemma        | gsdsimp_nocharlm |\n",
      "| constituency | ctb-51_charlm    |\n",
      "| depparse     | gsdsimp_charlm   |\n",
      "| sentiment    | ren_charlm       |\n",
      "| ner          | ontonotes        |\n",
      "===================================\n",
      "\n",
      "2024-09-17 11:07:22 INFO: Using device: cuda\n",
      "2024-09-17 11:07:22 INFO: Loading: tokenize\n",
      "2024-09-17 11:07:23 INFO: Loading: pos\n",
      "2024-09-17 11:07:23 INFO: Loading: lemma\n",
      "2024-09-17 11:07:23 INFO: Loading: constituency\n",
      "2024-09-17 11:07:24 INFO: Loading: depparse\n",
      "2024-09-17 11:07:24 INFO: Loading: sentiment\n",
      "2024-09-17 11:07:24 INFO: Loading: ner\n",
      "2024-09-17 11:07:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tree import Tree\n",
    "from dep_func import depTripleFuncLex\n",
    "import json\n",
    "import tqdm\n",
    "# nlp_constituency = stanza.Pipeline('zh', processors='tokenize,pos,constituency')\n",
    "nlp_dependency = stanza.Pipeline('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constituency_parsing(sentence):\n",
    "    doc = nlp_dependency(sentence)\n",
    "    sentence_constituency = str(doc.sentences[0].constituency)\n",
    "    \n",
    "    return sentence_constituency\n",
    "\n",
    "\n",
    "#1.0 NP→PN\n",
    "def NP_PN(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 1:\n",
    "        if t[0].label() == 'PN':\n",
    "            vector[0] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_PN(subtree,vector)\n",
    "\n",
    "# 4.0 NP→DP NP\n",
    "def NP_DP_NP(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 2:\n",
    "        if t[0].label() == 'DP' and t[1].label() == 'NP':\n",
    "            vector[1] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_DP_NP(subtree,vector)\n",
    "\n",
    "# 6.2 DP→DT\n",
    "def DP_DT(t,vector):\n",
    "    if t.label() == 'DP' and len(t) >= 1:\n",
    "        if t[0].label() == 'DT':\n",
    "            vector[2] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            DP_DT(subtree,vector)\n",
    "\n",
    "#6.6 IP→NP VP PU\n",
    "def IP_NP_VP_PU(t,vector):\n",
    "    if t.label() == 'IP' and len(t) >= 3:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'VP' and t[2].label() == 'PU':\n",
    "            vector[3] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            IP_NP_VP_PU(subtree,vector)\n",
    "\n",
    "#6.8 PRN→PU NP PU\n",
    "def PRN_PU_NP_PU(t,vector):\n",
    "    if t.label() == 'PRN' and len(t) >= 3:\n",
    "        if t[0].label() == 'PU' and t[1].label() == 'NP' and t[2].label() == 'PU':\n",
    "            vector[4] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            PRN_PU_NP_PU(subtree,vector)\n",
    "\n",
    "#6.8 NP→NR\n",
    "def NP_NR(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 1:\n",
    "        if t[0].label() == 'NR':\n",
    "            vector[5] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NR(subtree,vector)\n",
    "\n",
    "#10.0 CP_ADVP_IP\n",
    "def CP_ADVP_IP(t,vector):\n",
    "    if t.label() == 'CP' and len(t) >= 2:\n",
    "        if t[0].label() == 'ADVP' and t[1].label() == 'IP':\n",
    "            vector[6] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            CP_ADVP_IP(subtree,vector)\n",
    "\n",
    "#10.6 NP_DNP_NP\n",
    "def NP_DNP_NP(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 2:\n",
    "        if t[0].label() == 'DNP' and t[1].label() == 'NP':\n",
    "            vector[7] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_DNP_NP(subtree,vector)\n",
    "            \n",
    "#16.4 ADVP_CS\n",
    "def ADVP_CS(t,vector):\n",
    "    if t.label() == 'ADVP' and len(t) >= 1:\n",
    "        if t[0].label() == 'CS':\n",
    "            vector[8] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            ADVP_CS(subtree,vector)\n",
    "\n",
    "#16.8 DNP_NP_DEG\n",
    "def DNP_NP_DEG(t,vector):\n",
    "    if t.label() == 'DNP' and len(t) >= 2:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'DEG': \n",
    "            vector[9] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            DNP_NP_DEG(subtree,vector)\n",
    "\n",
    "def NP_QP_DNP_NP(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 3:\n",
    "        if t[0].label() == 'QP' and t[1].label() == 'DNP' and t[2].label() == 'NP':\n",
    "            vector[10] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_QP_DNP_NP(subtree,vector)\n",
    "\n",
    "def NP_NP_PRN(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 2:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'PRN':\n",
    "            vector[11] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NP_PRN(subtree,vector)\n",
    "\n",
    "def NP_NR_CC_NR(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 3:\n",
    "        if t[0].label() == 'NR' and t[1].label() == 'CC' and t[2].label() == 'NR':\n",
    "            vector[12] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NR_CC_NR(subtree,vector)\n",
    "\n",
    "def NP_NP_CC_NP(t,vector):\n",
    "    global score, count\n",
    "    if t.label() == 'NP' and len(t) >= 3:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'CC' and t[2].label() == 'NP':\n",
    "            vector[13] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NP_CC_NP(subtree,vector)\n",
    "            \n",
    "def get_feature_vector(sentence,vector):\n",
    "    \n",
    "    sentence_constituency = get_constituency_parsing(sentence)\n",
    "    #CFGR features\n",
    "    tree = Tree.fromstring(sentence_constituency)\n",
    "    find_NP_PN = NP_PN(tree,vector)\n",
    "    find_NP_DP_NP = NP_DP_NP(tree,vector)\n",
    "    find_DP_DT = DP_DT(tree,vector)\n",
    "    find_IP_NP_VP_PU = IP_NP_VP_PU(tree,vector)\n",
    "    find_PRN_PU_NP_PU = PRN_PU_NP_PU(tree,vector)\n",
    "    find_NP_NR = NP_NR(tree,vector)\n",
    "    find_CP_ADVP_IP = CP_ADVP_IP(tree,vector)\n",
    "    find_NP_DNP_NP = NP_DNP_NP(tree,vector)\n",
    "    find_ADVP_CS = ADVP_CS(tree,vector)\n",
    "    find_DNP_NP_DEG = DNP_NP_DEG(tree,vector)\n",
    "    #NP features\n",
    "    find_NP_QP_DNP_NP = NP_QP_DNP_NP(tree,vector)\n",
    "    find_NP_NP_PRN = NP_NP_PRN(tree,vector)\n",
    "    find_NP_NR_CC_NR = NP_NR_CC_NR(tree,vector)\n",
    "    find_NP_NP_CC_NP = NP_NP_CC_NP(tree,vector)\n",
    "\n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_parsing(sentences):\n",
    "    parsed_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()  # 去除多余的空白字符\n",
    "        if not sentence:\n",
    "            continue\n",
    "        doc_dependency = nlp_dependency(sentence)\n",
    "        parsed_words.append(doc_dependency)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_vector_dep(sentence,vector):\n",
    "    doc_dependency = nlp_dependency(sentence)\n",
    "    a = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','我')\n",
    "    b = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','advmod','将')\n",
    "    c = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','他')\n",
    "    d = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','det','该')\n",
    "    e = depTripleFuncLex(doc_dependency.sentences[0].words,'NR','case','的')\n",
    "    f = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','他们')\n",
    "    g = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','她')\n",
    "    h = depTripleFuncLex(doc_dependency.sentences[0].words,'他','case','的')\n",
    "    i = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','nmod:assmod','他')\n",
    "    j = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','punct','。')\n",
    "    k = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','advmod','但是')\n",
    "    l = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','你')\n",
    "    m = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','advmod','如果')\n",
    "    n = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','mark','的')\n",
    "    o = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','det','任何')\n",
    "    p = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','case','因为')\n",
    "    q = depTripleFuncLex(doc_dependency.sentences[0].words,'NR','cc','和')\n",
    "    r = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','det','那些')\n",
    "    s = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','它')\n",
    "    t = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','dobj','它')\n",
    "    if a:\n",
    "        vector[14] = 1\n",
    "    if b:\n",
    "        vector[15] = 1\n",
    "    if c:\n",
    "        vector[16] = 1\n",
    "    if d:\n",
    "        vector[17] = 1\n",
    "    if e:\n",
    "        vector[18] = 1\n",
    "    if f:\n",
    "        vector[19] = 1\n",
    "    if g:\n",
    "        vector[20] = 1\n",
    "    if h:\n",
    "        vector[21] = 1\n",
    "    if i:\n",
    "        vector[22] = 1\n",
    "    if j:\n",
    "        vector[23] = 1\n",
    "    if k:\n",
    "        vector[24] = 1\n",
    "    if l:\n",
    "        vector[25] = 1\n",
    "    if m:\n",
    "        vector[26] = 1\n",
    "    if n:\n",
    "        vector[27] = 1\n",
    "    if o:\n",
    "        vector[28] = 1\n",
    "    if p:\n",
    "        vector[29] = 1\n",
    "    if q:   \n",
    "        vector[30] = 1\n",
    "    if r:\n",
    "        vector[31] = 1\n",
    "    if s:\n",
    "        vector[32] = 1\n",
    "    if t:\n",
    "        vector[33] = 1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [02:56<00:00,  3.94it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file = open('processed/cluster0_better_translation.json', 'r', encoding='utf-8')\n",
    "data = json.load(file)\n",
    "vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "\n",
    "\n",
    "# for i,dic in tqdm(enumerate(data), ):\n",
    "result = []\n",
    "for i, dic in tqdm.tqdm(enumerate(data), total=len(data)):\n",
    "    sentence = data[i]\n",
    "    tmp = get_feature_vector(sentence,vector)\n",
    "    tmp = get_feature_vector_dep(sentence,vector)\n",
    "    str_tmp = ''.join(str(x) for x in tmp)\n",
    "    vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    result.append({'zh':sentence, 'vec':str_tmp})\n",
    "\n",
    "with open('processed/cluster0_better_transfvec.json','w', encoding='utf-8') as f:\n",
    "    json.dump(result,f,ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type set is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\VisualStudioProjects\\NLP\\src\\fvec.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/VisualStudioProjects/NLP/src/fvec.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprocessed/cluster0_better_transfvec.json\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/VisualStudioProjects/NLP/src/fvec.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     json\u001b[39m.\u001b[39;49mdump(result,f,ensure_ascii\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;49;00m chunk \u001b[39min\u001b[39;49;00m iterable:\n\u001b[0;32m    180\u001b[0m     fp\u001b[39m.\u001b[39;49mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[39myield\u001b[39;00m _floatstr(o)\n\u001b[0;32m    429\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 430\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    431\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    432\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 326\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    327\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    328\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    438\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[1;32m--> 439\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[0;32m    440\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    441\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    162\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m \n\u001b[0;32m    179\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    181\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type set is not JSON serializable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/2024-09-14-21-42-57_processed.json','w', encoding='utf-8') as f:\n",
    "    json.dump(data,f,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
