{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定运行环境，是cuda还是cpu\n",
    "ENV = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    ENV=\"cuda\"\n",
    "else:\n",
    "    ENV=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型和分词器的初始化\n",
    "origin_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\").to(ENV)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"en_XX\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_forward_func = origin_model.model.decoder.layers[11].forward\n",
    "\n",
    "random_number = random.uniform(-1, 1)\n",
    "t = 1\n",
    "lasttoken_pos = 0\n",
    "last_token_state = torch.zeros(5, 1, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def new_decoder_activate(hidden_states, attention_mask = None, encoder_hidden_states = None, encoder_attention_mask = None,\n",
    "                         layer_head_mask = None, cross_attn_layer_head_mask = None, past_key_value = None, output_attentions = False,\n",
    "                         use_cache = True):\n",
    "    global last_token_state\n",
    "    modified_hidden_states = hidden_states  # 加随机数 torch.nn.functional.gelu\n",
    "    last_token_state = modified_hidden_states\n",
    "    # 调用原始 decoder 层的 forward 方法\n",
    "    output = decoder_forward_func(\n",
    "        modified_hidden_states, attention_mask, encoder_hidden_states,\n",
    "        encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask,\n",
    "        past_key_value, output_attentions, use_cache\n",
    "    )\n",
    "    # final_output = output * 2  # 举例，对输出进行缩放\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活模型\n",
    "if 'activated_model' in globals():\n",
    "    del activated_model\n",
    "    # 清理未使用的显存\n",
    "    torch.cuda.empty_cache()\n",
    "    # 强制垃圾回收\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "activated_model = deepcopy(origin_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activated_model.model.decoder.layers[11].forward = new_decoder_activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695\n"
     ]
    }
   ],
   "source": [
    "cluster0_encodes = []\n",
    "with open('.\\\\processed\\\\cluster0_better.json', 'r', encoding='utf-8') as file:\n",
    "    cluster = json.load(file)\n",
    "cluster0 = cluster\n",
    "for item in cluster0:\n",
    "    tmp_encoded_input = tokenizer(item, return_tensors=\"pt\").to(ENV)\n",
    "    cluster0_encodes.append(tmp_encoded_input)\n",
    "print(len(cluster0_encodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list1(l):\n",
    "    sum =0\n",
    "    for i in l:\n",
    "        sum+=i\n",
    "    return sum\n",
    "\n",
    "def str1(s):\n",
    "    sum = 0\n",
    "    for c in s:\n",
    "        sum+=int(c)\n",
    "    return sum\n",
    "\n",
    "def l2s(tmp):\n",
    "    return ''.join(str(x) for x in tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 695/695 [03:50<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "cluster0_origin = torch.zeros(5, 1, 1024).to(ENV)\n",
    "for index, encoded_input in tqdm(enumerate(cluster0_encodes), total=len(cluster0_encodes)):\n",
    "    i=1\n",
    "    while i > 0:\n",
    "        t = 1\n",
    "        generated_tokens = activated_model.generate(\n",
    "            **encoded_input, \n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "        )\n",
    "        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        i-=1\n",
    "    cluster0_origin += last_token_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 当前时间\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "with open(f'processed/origin{current_time}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump((cluster0_origin/len(cluster0_encodes)).tolist(), file, ensure_ascii=False, indent=4, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'processed/cluster0_steer.json', 'r', encoding='utf-8') as file:\n",
    "    vv = json.load(file)\n",
    "torch_vv = torch.tensor(vv)\n",
    "torch_vv/=695\n",
    "with open(f'processed/cluster0_steer{current_time}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump((torch_vv).tolist(), file, ensure_ascii=False, indent=4, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_steer_vector = torch_vv.to(ENV) - (cluster0_origin/len(cluster0_encodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'processed/cluster0_steer_final{current_time}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump((final_steer_vector).tolist(), file, ensure_ascii=False, indent=4, separators=(',', ':'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
