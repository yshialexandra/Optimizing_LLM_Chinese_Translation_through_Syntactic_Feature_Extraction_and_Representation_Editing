{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import stanza\n",
    "from nltk.tree import Tree\n",
    "from dep_func import depTripleFuncLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 13:58:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4c599d999f421ba0a36039192e6be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-16 13:58:37 INFO: Downloaded file to C:\\Users\\Administrator\\stanza_resources\\resources.json\n",
      "2024-09-16 13:58:37 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-09-16 13:58:38 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "===================================\n",
      "| Processor    | Package          |\n",
      "-----------------------------------\n",
      "| tokenize     | gsdsimp          |\n",
      "| pos          | gsdsimp_charlm   |\n",
      "| lemma        | gsdsimp_nocharlm |\n",
      "| constituency | ctb-51_charlm    |\n",
      "| depparse     | gsdsimp_charlm   |\n",
      "| sentiment    | ren_charlm       |\n",
      "| ner          | ontonotes        |\n",
      "===================================\n",
      "\n",
      "2024-09-16 13:58:38 INFO: Using device: cuda\n",
      "2024-09-16 13:58:38 INFO: Loading: tokenize\n",
      "2024-09-16 13:58:38 INFO: Loading: pos\n",
      "2024-09-16 13:58:39 INFO: Loading: lemma\n",
      "2024-09-16 13:58:39 INFO: Loading: constituency\n",
      "2024-09-16 13:58:39 INFO: Loading: depparse\n",
      "2024-09-16 13:58:39 INFO: Loading: sentiment\n",
      "2024-09-16 13:58:40 INFO: Loading: ner\n",
      "2024-09-16 13:58:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_dependency = stanza.Pipeline('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_constituency_parsing(sentence):\n",
    "    doc = nlp_dependency(sentence)\n",
    "    sentence_constituency = str(doc.sentences[0].constituency)\n",
    "    \n",
    "    return sentence_constituency\n",
    "\n",
    "\n",
    "#1.0 NP→PN\n",
    "def NP_PN(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 1:\n",
    "        if t[0].label() == 'PN':\n",
    "            vector[0] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_PN(subtree,vector)\n",
    "\n",
    "# 4.0 NP→DP NP\n",
    "def NP_DP_NP(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 2:\n",
    "        if t[0].label() == 'DP' and t[1].label() == 'NP':\n",
    "            vector[1] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_DP_NP(subtree,vector)\n",
    "\n",
    "# 6.2 DP→DT\n",
    "def DP_DT(t,vector):\n",
    "    if t.label() == 'DP' and len(t) >= 1:\n",
    "        if t[0].label() == 'DT':\n",
    "            vector[2] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            DP_DT(subtree,vector)\n",
    "\n",
    "#6.6 IP→NP VP PU\n",
    "def IP_NP_VP_PU(t,vector):\n",
    "    if t.label() == 'IP' and len(t) >= 3:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'VP' and t[2].label() == 'PU':\n",
    "            vector[3] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            IP_NP_VP_PU(subtree,vector)\n",
    "\n",
    "#6.8 PRN→PU NP PU\n",
    "def PRN_PU_NP_PU(t,vector):\n",
    "    if t.label() == 'PRN' and len(t) >= 3:\n",
    "        if t[0].label() == 'PU' and t[1].label() == 'NP' and t[2].label() == 'PU':\n",
    "            vector[4] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            PRN_PU_NP_PU(subtree,vector)\n",
    "\n",
    "#6.8 NP→NR\n",
    "def NP_NR(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 1:\n",
    "        if t[0].label() == 'NR':\n",
    "            vector[5] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NR(subtree,vector)\n",
    "\n",
    "#10.0 CP_ADVP_IP\n",
    "def CP_ADVP_IP(t,vector):\n",
    "    if t.label() == 'CP' and len(t) >= 2:\n",
    "        if t[0].label() == 'ADVP' and t[1].label() == 'IP':\n",
    "            vector[6] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            CP_ADVP_IP(subtree,vector)\n",
    "\n",
    "#10.6 NP_DNP_NP\n",
    "def NP_DNP_NP(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 2:\n",
    "        if t[0].label() == 'DNP' and t[1].label() == 'NP':\n",
    "            vector[7] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_DNP_NP(subtree,vector)\n",
    "            \n",
    "#16.4 ADVP_CS\n",
    "def ADVP_CS(t,vector):\n",
    "    if t.label() == 'ADVP' and len(t) >= 1:\n",
    "        if t[0].label() == 'CS':\n",
    "            vector[8] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            ADVP_CS(subtree,vector)\n",
    "\n",
    "#16.8 DNP_NP_DEG\n",
    "def DNP_NP_DEG(t,vector):\n",
    "    if t.label() == 'DNP' and len(t) >= 2:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'DEG': \n",
    "            vector[9] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            DNP_NP_DEG(subtree,vector)\n",
    "\n",
    "def NP_QP_DNP_NP(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 3:\n",
    "        if t[0].label() == 'QP' and t[1].label() == 'DNP' and t[2].label() == 'NP':\n",
    "            vector[10] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_QP_DNP_NP(subtree,vector)\n",
    "\n",
    "def NP_NP_PRN(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 2:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'PRN':\n",
    "            vector[11] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NP_PRN(subtree,vector)\n",
    "\n",
    "def NP_NR_CC_NR(t,vector):\n",
    "    if t.label() == 'NP' and len(t) >= 3:\n",
    "        if t[0].label() == 'NR' and t[1].label() == 'CC' and t[2].label() == 'NR':\n",
    "            vector[12] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NR_CC_NR(subtree,vector)\n",
    "\n",
    "def NP_NP_CC_NP(t,vector):\n",
    "    global score, count\n",
    "    if t.label() == 'NP' and len(t) >= 3:\n",
    "        if t[0].label() == 'NP' and t[1].label() == 'CC' and t[2].label() == 'NP':\n",
    "            vector[13] = 1\n",
    "    for subtree in t:\n",
    "        if isinstance(subtree, Tree):\n",
    "            NP_NP_CC_NP(subtree,vector)\n",
    "            \n",
    "def get_feature_vector(sentence,vector):\n",
    "    \n",
    "    sentence_constituency = get_constituency_parsing(sentence)\n",
    "    #CFGR features\n",
    "    tree = Tree.fromstring(sentence_constituency)\n",
    "    find_NP_PN = NP_PN(tree,vector)\n",
    "    find_NP_DP_NP = NP_DP_NP(tree,vector)\n",
    "    find_DP_DT = DP_DT(tree,vector)\n",
    "    find_IP_NP_VP_PU = IP_NP_VP_PU(tree,vector)\n",
    "    find_PRN_PU_NP_PU = PRN_PU_NP_PU(tree,vector)\n",
    "    find_NP_NR = NP_NR(tree,vector)\n",
    "    find_CP_ADVP_IP = CP_ADVP_IP(tree,vector)\n",
    "    find_NP_DNP_NP = NP_DNP_NP(tree,vector)\n",
    "    find_ADVP_CS = ADVP_CS(tree,vector)\n",
    "    find_DNP_NP_DEG = DNP_NP_DEG(tree,vector)\n",
    "    #NP features\n",
    "    find_NP_QP_DNP_NP = NP_QP_DNP_NP(tree,vector)\n",
    "    find_NP_NP_PRN = NP_NP_PRN(tree,vector)\n",
    "    find_NP_NR_CC_NR = NP_NR_CC_NR(tree,vector)\n",
    "    find_NP_NP_CC_NP = NP_NP_CC_NP(tree,vector)\n",
    "\n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dependency_parsing(sentences):\n",
    "    parsed_words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()  # 去除多余的空白字符\n",
    "        if not sentence:\n",
    "            continue\n",
    "        doc_dependency = nlp_dependency(sentence)\n",
    "        parsed_words.append(doc_dependency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_feature_vector_dep(sentence,vector):\n",
    "    doc_dependency = nlp_dependency(sentence)\n",
    "    a = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','我')\n",
    "    b = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','advmod','将')\n",
    "    c = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','他')\n",
    "    d = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','det','该')\n",
    "    e = depTripleFuncLex(doc_dependency.sentences[0].words,'NR','case','的')\n",
    "    f = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','他们')\n",
    "    g = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','她')\n",
    "    h = depTripleFuncLex(doc_dependency.sentences[0].words,'他','case','的')\n",
    "    i = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','nmod:assmod','他')\n",
    "    j = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','punct','。')\n",
    "    k = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','advmod','但是')\n",
    "    l = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','你')\n",
    "    m = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','advmod','如果')\n",
    "    n = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','mark','的')\n",
    "    o = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','det','任何')\n",
    "    p = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','case','因为')\n",
    "    q = depTripleFuncLex(doc_dependency.sentences[0].words,'NR','cc','和')\n",
    "    r = depTripleFuncLex(doc_dependency.sentences[0].words,'NN','det','那些')\n",
    "    s = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','nsubj','它')\n",
    "    t = depTripleFuncLex(doc_dependency.sentences[0].words,'VV','dobj','它')\n",
    "    if a:\n",
    "        vector[14] = 1\n",
    "    if b:\n",
    "        vector[15] = 1\n",
    "    if c:\n",
    "        vector[16] = 1\n",
    "    if d:\n",
    "        vector[17] = 1\n",
    "    if e:\n",
    "        vector[18] = 1\n",
    "    if f:\n",
    "        vector[19] = 1\n",
    "    if g:\n",
    "        vector[20] = 1\n",
    "    if h:\n",
    "        vector[21] = 1\n",
    "    if i:\n",
    "        vector[22] = 1\n",
    "    if j:\n",
    "        vector[23] = 1\n",
    "    if k:\n",
    "        vector[24] = 1\n",
    "    if l:\n",
    "        vector[25] = 1\n",
    "    if m:\n",
    "        vector[26] = 1\n",
    "    if n:\n",
    "        vector[27] = 1\n",
    "    if o:\n",
    "        vector[28] = 1\n",
    "    if p:\n",
    "        vector[29] = 1\n",
    "    if q:   \n",
    "        vector[30] = 1\n",
    "    if r:\n",
    "        vector[31] = 1\n",
    "    if s:\n",
    "        vector[32] = 1\n",
    "    if t:\n",
    "        vector[33] = 1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定运行环境，是cuda还是cpu\n",
    "ENV = \"\"\n",
    "if torch.cuda.is_available():\n",
    "    ENV=\"cuda\"\n",
    "else:\n",
    "    ENV=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型和分词器的初始化\n",
    "origin_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\").to(ENV)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"en_XX\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_forward_func = origin_model.model.decoder.layers[11].forward\n",
    "\n",
    "random_number = random.uniform(-1, 1)\n",
    "t = 1\n",
    "lasttoken_pos = 0\n",
    "last_token_state = torch.zeros(5, 1, 1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def new_decoder_activate(hidden_states, attention_mask = None, encoder_hidden_states = None, encoder_attention_mask = None,\n",
    "                         layer_head_mask = None, cross_attn_layer_head_mask = None, past_key_value = None, output_attentions = False,\n",
    "                         use_cache = True):\n",
    "    global t\n",
    "    global last_token_state\n",
    "    if t % 4 == 0:\n",
    "        random_number = random.uniform(-1, 1)\n",
    "        modified_hidden_states = hidden_states\n",
    "        modified_hidden_states += random_number\n",
    "        modified_hidden_states = torch.nn.functional.gelu(modified_hidden_states)\n",
    "    else:\n",
    "        modified_hidden_states = hidden_states  # 加随机数 torch.nn.functional.gelu\n",
    "    t+=1\n",
    "    t%=4\n",
    "    last_token_state = modified_hidden_states\n",
    "    # 调用原始 decoder 层的 forward 方法\n",
    "    output = decoder_forward_func(\n",
    "        modified_hidden_states, attention_mask, encoder_hidden_states,\n",
    "        encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask,\n",
    "        past_key_value, output_attentions, use_cache\n",
    "    )\n",
    "    # final_output = output * 2  # 举例，对输出进行缩放\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活模型\n",
    "if 'activated_model' in globals():\n",
    "    del activated_model\n",
    "    # 清理未使用的显存\n",
    "    torch.cuda.empty_cache()\n",
    "    # 强制垃圾回收\n",
    "    gc.collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "activated_model = deepcopy(origin_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "activated_model.model.decoder.layers[11].forward = new_decoder_activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster0_encodes = []\n",
    "with open('.\\\\processed\\\\2024-09-16-13-39-51_clustered.json', 'r', encoding='utf-8') as file:\n",
    "    cluster = json.load(file)\n",
    "cluster0 = cluster['0']\n",
    "for item in cluster0:\n",
    "    tmp_encoded_input = tokenizer(item['en'], return_tensors=\"pt\").to(ENV)\n",
    "    cluster0_encodes.append(tmp_encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list1(l):\n",
    "    sum =0\n",
    "    for i in l:\n",
    "        sum+=i\n",
    "    return sum\n",
    "\n",
    "def str1(s):\n",
    "    sum = 0\n",
    "    for c in s:\n",
    "        sum+=int(c)\n",
    "    return sum\n",
    "\n",
    "def l2s(tmp):\n",
    "    return ''.join(str(x) for x in tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sens=[]\n",
    "cluster0_steer = torch.zeros(5, 1, 1024).to(ENV)\n",
    "for index, encoded_input in enumerate(cluster0_encodes):\n",
    "    i=10\n",
    "    zh_tmp = []\n",
    "    fvec_list_tmp = []\n",
    "    better = 0\n",
    "    last_token_hidden_states = torch.zeros(5, 1, 1024).to(ENV)\n",
    "    while i > 0:\n",
    "        t = 1\n",
    "        generated_tokens = activated_model.generate(\n",
    "            **encoded_input, \n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "        )\n",
    "        translated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "        translated_text = translated_text.replace(\" \", \"\")\n",
    "        fvc_tmp = get_feature_vector(translated_text,vector)\n",
    "        fvc_tmp = get_feature_vector_dep(translated_text,vector)\n",
    "        vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        if list1(fvc_tmp) < str1(cluster0[index]['fvec']):\n",
    "            better+=1\n",
    "            zh_tmp.append(translated_text)\n",
    "            last_token_hidden_states += last_token_state\n",
    "            fvec_list_tmp.append(l2s(fvc_tmp))\n",
    "        i-=1\n",
    "    if better > 0:\n",
    "        cluster0_steer += last_token_hidden_states / better\n",
    "    translated_sens.append({'en': cluster0[index]['en'], 'baseline': cluster0[index]['zh'], 'zh': zh_tmp , 'fvec': fvec_list_tmp})\n",
    "    if (index + 1) % 10 == 0:\n",
    "        # 当前时间\n",
    "        current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        # 保存翻译结果\n",
    "        with open(f'output/activated{current_time}.json', 'w', encoding='utf-8') as file:\n",
    "            json.dump(translated_sens, file, ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "        translated_sens=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-941.3407, 3284.5481,  935.6570,  ..., 1477.7037, -390.3083,\n",
      "          -451.2839]],\n",
      "\n",
      "        [[-863.5672, 3005.5312,  898.7526,  ..., 1535.2255, -390.3169,\n",
      "          -416.3390]],\n",
      "\n",
      "        [[-358.0058, 2765.7063,  823.9050,  ..., 1495.9956, -547.4582,\n",
      "          -400.3140]],\n",
      "\n",
      "        [[ -83.2323, 2508.0881,  759.8983,  ..., 1456.4004, -645.4664,\n",
      "          -293.4832]],\n",
      "\n",
      "        [[ 191.5240, 2308.0579,  728.5186,  ..., 1427.7723, -796.5109,\n",
      "          -269.1384]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(cluster0_steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cluster0_steer.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(cluster0_steer.tolist(), file, ensure_ascii=False, indent=4, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 当前时间\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "with open(f'output/activated{current_time}.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(translated_sens, file, ensure_ascii=False, indent=4, separators=(',', ':'))\n",
    "translated_sens=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
