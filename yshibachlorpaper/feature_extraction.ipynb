{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions and data structures for feature extraction\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "#构造嵌套字典\n",
    "def tree():\n",
    "    return defaultdict(tree)\n",
    "#去掉空格\n",
    "def split_string(text):\n",
    "    result = []\n",
    "    tmp = \"\"\n",
    "    flag = 0\n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            flag = 0\n",
    "            if tmp != \"\":\n",
    "                result.append(tmp)\n",
    "                tmp = \"\"\n",
    "            continue\n",
    "        elif char == '(':\n",
    "            flag = 1\n",
    "            result.append(char)\n",
    "            continue\n",
    "        elif char == ')':\n",
    "            result.append(char)\n",
    "            continue\n",
    "        elif flag == 1:\n",
    "            tmp += char\n",
    "            continue\n",
    "    return result, ''.join(result)\n",
    "\n",
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def is_empty(self):\n",
    "        return self.items == []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items.pop()\n",
    "        else:\n",
    "            print(\"Stack is empty\")\n",
    "\n",
    "    def top(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items[-1]\n",
    "        else:\n",
    "            print(\"Stack is empty\")\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.items)\n",
    "\n",
    "#将递归的 defaultdict 转换为普通的嵌套字典。\n",
    "def dicts(t): return {k: dicts(t[k]) for k in t}\n",
    "\n",
    "def build_constituency_feature_tree(text, MAXLAYER=3):\n",
    "    treeStrList, tmp = split_string(text)\n",
    "    featureTree = tree()\n",
    "    stack = Stack()\n",
    "    i = 0\n",
    "    layer = -1\n",
    "    while i < len(treeStrList):\n",
    "        if treeStrList[i] == '(':\n",
    "            i+=1\n",
    "            layer+=1\n",
    "            if layer <= MAXLAYER:\n",
    "                if(stack.is_empty()):\n",
    "                    featureTree[treeStrList[i]]\n",
    "                    stack.push(featureTree[treeStrList[i]])\n",
    "                else:\n",
    "                    if(treeStrList[i] in stack.top()):\n",
    "                        keytmp = treeStrList[i] + str(random.randint(1, 100000)) #发现重复键，则附加随机数\n",
    "                    else:\n",
    "                        keytmp = treeStrList[i]\n",
    "                    stack.top()[keytmp]\n",
    "                    stack.push(stack.top()[keytmp])\n",
    "            i+=1\n",
    "            continue\n",
    "        if treeStrList[i] == ')':\n",
    "            if layer > 0 and layer <= MAXLAYER:\n",
    "                stack.pop()\n",
    "            layer-=1\n",
    "            i+=1\n",
    "            continue\n",
    "    return dicts(featureTree)['ROOT']\n",
    "\n",
    "def extract_constituency_feature_from_tree(feature_tree):\n",
    "    queue = []\n",
    "    result = []\n",
    "    queue.append(feature_tree)\n",
    "    while len(queue) > 0:\n",
    "        tmp = queue.pop()\n",
    "        for key in tmp.keys():\n",
    "            if isinstance(tmp[key], dict) and len(tmp[key]) > 0:\n",
    "                queue.append(tmp[key])\n",
    "                # 去除数字\n",
    "                result.append(''.join(char for char in str({key: tmp[key]}) if not char.isdigit()))\n",
    "    return result\n",
    "\n",
    "def extract_dependency_feature_from_list(words):\n",
    "    sent_list = []\n",
    "    for word in words:\n",
    "        head_id = word.head\n",
    "        head_upos = \"ROOT\" if head_id == 0 else words[head_id - 1].upos  # 获取 head 的 UPOS\n",
    "        pos_list = [word.upos, head_upos, word.deprel]\n",
    "        head_lemma = \"NONE\" if head_id == 0 else words[head_id - 1].lemma\n",
    "        lex_list = [word.lemma, head_lemma, word.deprel]\n",
    "        sent_list.append(f'{pos_list}\\t{lex_list}')\n",
    "    return sent_list\n",
    "\n",
    "def parse_tmx(file):\n",
    "    # 解析 XML 文件\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 找到 TMX 文件中的 body 部分\n",
    "    body = root.find(\"body\")\n",
    "    if body is None:\n",
    "        raise ValueError(\"Invalid TMX file: missing <body> section.\")\n",
    "\n",
    "    # 遍历所有 <tu> 元素\n",
    "    for tu in body.findall(\"tu\"):\n",
    "        translations = {}\n",
    "        # 提取每个 <tu> 中的 <tuv> 元素\n",
    "        for tuv in tu.findall(\"tuv\"):\n",
    "            lang = tuv.attrib.get(\"{http://www.w3.org/XML/1998/namespace}lang\")  # 获取语言属性\n",
    "            seg = tuv.find(\"seg\")\n",
    "            if lang and seg is not None:\n",
    "                translations[lang] = seg.text.strip()\n",
    "\n",
    "        # 如果有源语言和目标语言，返回一对\n",
    "        if \"en-US\" in translations and \"zh-CN\" in translations:\n",
    "            yield translations[\"en-US\"], translations[\"zh-CN\"]\n",
    "\n",
    "def list_txt_files(folder_path):\n",
    "    result = []\n",
    "    # 遍历文件夹及其子文件夹中的所有文件\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):  # 只处理以 .txt 结尾的文件\n",
    "                file_path = os.path.join(root, file)\n",
    "                result.append(file_path)  # 返回文件的完整路径\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 19:52:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6026e58b56433599ba6b8cf22ba8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 19:52:10 INFO: Downloaded file to /Users/yishi/stanza_resources/resources.json\n",
      "2024-12-24 19:52:10 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2024-12-24 19:52:13 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "===================================\n",
      "| Processor    | Package          |\n",
      "-----------------------------------\n",
      "| tokenize     | gsdsimp          |\n",
      "| pos          | gsdsimp_charlm   |\n",
      "| lemma        | gsdsimp_nocharlm |\n",
      "| constituency | ctb-51_charlm    |\n",
      "| depparse     | gsdsimp_charlm   |\n",
      "| sentiment    | ren_charlm       |\n",
      "| ner          | ontonotes        |\n",
      "===================================\n",
      "\n",
      "2024-12-24 19:52:13 INFO: Using device: cpu\n",
      "2024-12-24 19:52:13 INFO: Loading: tokenize\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:13 INFO: Loading: pos\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:13 INFO: Loading: lemma\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:13 INFO: Loading: constituency\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:14 INFO: Loading: depparse\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:14 INFO: Loading: sentiment\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:14 INFO: Loading: ner\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-24 19:52:16 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# stanza pipeline for constituency and dependency parsing\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count_map = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sentences_map = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./instruct_txt\\\\A01A_en_zh.txt', './instruct_txt\\\\A01B_en_zh.txt', './instruct_txt\\\\A02A_en_zh.txt', './instruct_txt\\\\A02B_en_zh.txt', './instruct_txt\\\\A02C_en_zh.txt', './instruct_txt\\\\A02D_en_zh.txt', './instruct_txt\\\\A03A_en_zh.txt', './instruct_txt\\\\A03B_en_zh.txt', './instruct_txt\\\\A04A_en_zh.txt', './instruct_txt\\\\A05A_en_zh.txt', './instruct_txt\\\\A07A_en_zh.txt', './instruct_txt\\\\A07B_en_zh.txt', './instruct_txt\\\\A08A_en_zh.txt', './instruct_txt\\\\A08B_en_zh.txt', './instruct_txt\\\\A09A_en_zh.txt', './instruct_txt\\\\A10A_en_zh.txt', './instruct_txt\\\\A11A_en_zh.txt', './instruct_txt\\\\A12A_en_zh.txt', './instruct_txt\\\\A13A_en_zh.txt', './instruct_txt\\\\A13B_en_zh.txt', './instruct_txt\\\\A14A_en_zh.txt', './instruct_txt\\\\A15B_en_zh.txt', './instruct_txt\\\\A16A_en_zh.txt', './instruct_txt\\\\A17A_en_zh.txt', './instruct_txt\\\\A18A_en_zh.txt', './instruct_txt\\\\A18B_en_zh.txt', './instruct_txt\\\\A18C_en_zh.txt', './instruct_txt\\\\A19A_en_zh.txt', './instruct_txt\\\\A19B_en_zh.txt', './instruct_txt\\\\A21A_en_zh.txt', './instruct_txt\\\\A21B_en_zh.txt', './instruct_txt\\\\A21C_en_zh.txt', './instruct_txt\\\\A23A_en_zh.txt', './instruct_txt\\\\A23B_en_zh.txt', './instruct_txt\\\\A23C_en_zh.txt', './instruct_txt\\\\A23D_en_zh.txt', './instruct_txt\\\\A24B_en_zh.txt', './instruct_txt\\\\A24C_en_zh.txt', './instruct_txt\\\\A25B_en_zh.txt', './instruct_txt\\\\A25C_en_zh.txt', './instruct_txt\\\\A26A_en_zh.txt', './instruct_txt\\\\A26B_en_zh.txt', './instruct_txt\\\\A27A_en_zh.txt', './instruct_txt\\\\A28_en_zh.txt', './instruct_txt\\\\A30A_en_zh.txt', './instruct_txt\\\\A31A_en_zh.txt', './instruct_txt\\\\A31B_en_zh.txt', './instruct_txt\\\\A33A_en_zh.txt', './instruct_txt\\\\A33B_en_zh.txt', './instruct_txt\\\\A33D_en_zh.txt', './instruct_txt\\\\A35A_en_zh.txt', './instruct_txt\\\\A36A_en_zh.txt', './instruct_txt\\\\A37B_en_zh.txt', './instruct_txt\\\\A40A_en_zh.txt', './instruct_txt\\\\A43A_en_zh.txt', './instruct_txt\\\\A44A_en_zh.txt', './instruct_txt\\\\B01A_en_zh.txt', './instruct_txt\\\\B02A_en_zh.txt', './instruct_txt\\\\B02B_en_zh.txt', './instruct_txt\\\\B03A_en_zh.txt', './instruct_txt\\\\B04A_en_zh.txt', './instruct_txt\\\\B04B_en_zh.txt', './instruct_txt\\\\B05A_en_zh.txt', './instruct_txt\\\\B06A_en_zh.txt', './instruct_txt\\\\B06B_en_zh.txt', './instruct_txt\\\\B07A_en_zh.txt', './instruct_txt\\\\B08A_en_zh.txt', './instruct_txt\\\\B13A_en_zh.txt', './instruct_txt\\\\B16A_en_zh.txt', './instruct_txt\\\\B17A_en_zh.txt', './instruct_txt\\\\B18A_en_zh.txt', './instruct_txt\\\\B18B_en_zh.txt', './instruct_txt\\\\B20A_en_zh.txt', './instruct_txt\\\\B20B_en_zh.txt', './instruct_txt\\\\B22A_en_zh.txt', './instruct_txt\\\\B27A_en_zh.txt', './instruct_txt\\\\B27B_en_zh.txt', './instruct_txt\\\\B27C_en_zh.txt', './instruct_txt\\\\C01A_en_zh.txt', './instruct_txt\\\\C02_en_zh.txt', './instruct_txt\\\\C03A_en_zh.txt', './instruct_txt\\\\C08A_en_zh.txt', './instruct_txt\\\\C09A_en_zh.txt', './instruct_txt\\\\C10A_en_zh.txt', './instruct_txt\\\\C11A_en_zh.txt', './instruct_txt\\\\C14A_en_zh.txt', './instruct_txt\\\\C15A_en_zh.txt', './instruct_txt\\\\C17A_en_zh.txt']\n"
     ]
    }
   ],
   "source": [
    "print(list_txt_files(\"./instruct_txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ll = []\n",
    "with open('./instruct_txt/A01A_en_zh.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    \n",
    "    \n",
    "    with open(f'tmp/test_feature.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./instruct_txt/C09A_en_zh.txt', './instruct_txt/A43A_en_zh.txt', './instruct_txt/A16A_en_zh.txt', './instruct_txt/A21A_en_zh.txt', './instruct_txt/B01A_en_zh.txt', './instruct_txt/A01B_en_zh.txt', './instruct_txt/C15A_en_zh.txt', './instruct_txt/A33D_en_zh.txt', './instruct_txt/B27A_en_zh.txt', './instruct_txt/A25C_en_zh.txt', './instruct_txt/A30A_en_zh.txt', './instruct_txt/A07A_en_zh.txt', './instruct_txt/B22A_en_zh.txt', './instruct_txt/A15B_en_zh.txt', './instruct_txt/C01A_en_zh.txt', './instruct_txt/A28_en_zh.txt', './instruct_txt/A02A_en_zh.txt', './instruct_txt/A35A_en_zh.txt', './instruct_txt/B02B_en_zh.txt', './instruct_txt/B18A_en_zh.txt', './instruct_txt/A18B_en_zh.txt', './instruct_txt/A13A_en_zh.txt', './instruct_txt/A33B_en_zh.txt', './instruct_txt/C10A_en_zh.txt', './instruct_txt/B04A_en_zh.txt', './instruct_txt/B03A_en_zh.txt', './instruct_txt/C17A_en_zh.txt', './instruct_txt/A03B_en_zh.txt', './instruct_txt/A08A_en_zh.txt', './instruct_txt/A23A_en_zh.txt', './instruct_txt/A14A_en_zh.txt', './instruct_txt/A05A_en_zh.txt', './instruct_txt/A19A_en_zh.txt', './instruct_txt/A25B_en_zh.txt', './instruct_txt/B17A_en_zh.txt', './instruct_txt/B20A_en_zh.txt', './instruct_txt/C03A_en_zh.txt', './instruct_txt/A18C_en_zh.txt', './instruct_txt/A31B_en_zh.txt', './instruct_txt/B06A_en_zh.txt', './instruct_txt/A44A_en_zh.txt', './instruct_txt/A11A_en_zh.txt', './instruct_txt/A26A_en_zh.txt', './instruct_txt/A23B_en_zh.txt', './instruct_txt/A21C_en_zh.txt', './instruct_txt/A03A_en_zh.txt', './instruct_txt/A08B_en_zh.txt', './instruct_txt/B08A_en_zh.txt', './instruct_txt/A19B_en_zh.txt', './instruct_txt/A12A_en_zh.txt', './instruct_txt/C11A_en_zh.txt', './instruct_txt/B05A_en_zh.txt', './instruct_txt/B27C_en_zh.txt', './instruct_txt/A02C_en_zh.txt', './instruct_txt/C08A_en_zh.txt', './instruct_txt/A17A_en_zh.txt', './instruct_txt/B20B_en_zh.txt', './instruct_txt/A23D_en_zh.txt', './instruct_txt/C14A_en_zh.txt', './instruct_txt/A37B_en_zh.txt', './instruct_txt/A26B_en_zh.txt', './instruct_txt/B06B_en_zh.txt', './instruct_txt/A31A_en_zh.txt', './instruct_txt/A24C_en_zh.txt', './instruct_txt/A23C_en_zh.txt', './instruct_txt/A36A_en_zh.txt', './instruct_txt/A01A_en_zh.txt', './instruct_txt/B16A_en_zh.txt', './instruct_txt/A21B_en_zh.txt', './instruct_txt/A02D_en_zh.txt', './instruct_txt/A07B_en_zh.txt', './instruct_txt/B07A_en_zh.txt', './instruct_txt/C02_en_zh.txt', './instruct_txt/B27B_en_zh.txt', './instruct_txt/A10A_en_zh.txt', './instruct_txt/A27A_en_zh.txt', './instruct_txt/B02A_en_zh.txt', './instruct_txt/A02B_en_zh.txt', './instruct_txt/A09A_en_zh.txt', './instruct_txt/A40A_en_zh.txt', './instruct_txt/B04B_en_zh.txt', './instruct_txt/A04A_en_zh.txt', './instruct_txt/A33A_en_zh.txt', './instruct_txt/A18A_en_zh.txt', './instruct_txt/B18B_en_zh.txt', './instruct_txt/A13B_en_zh.txt', './instruct_txt/A24B_en_zh.txt', './instruct_txt/B13A_en_zh.txt']\n"
     ]
    }
   ],
   "source": [
    "print(list_txt_files(\"./instruct_txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  25%|██▌       | 22/88 [08:34<25:43, 23.39s/file]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m     ll\u001b[38;5;241m.\u001b[39mappend(line)\n\u001b[0;32m----> 8\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Constituency\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(doc\u001b[38;5;241m.\u001b[39msentences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconstituency)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/ner_processor.py:114\u001b[0m, in \u001b[0;36mNERProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m    112\u001b[0m         preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[0;32m--> 114\u001b[0m             preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m         all_preds\u001b[38;5;241m.\u001b[39mappend(preds)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# for each sentence, gather a list of predictions\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# merge those predictions into a single list\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# earlier models will have precedence\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/ner/trainer.py:139\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, unsort)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m#batch_size = word.size(0)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m _, logits, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_orig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharoffsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_orig_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# decode\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# TODO: might need to decode multiple columns of output for\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# models with multiple layers\u001b[39;00m\n\u001b[1;32m    144\u001b[0m trans \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m trans]\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/ner/model.py:211\u001b[0m, in \u001b[0;36mNERTagger.forward\u001b[0;34m(self, sentences, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[1;32m    209\u001b[0m char_reps_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_forward\u001b[38;5;241m.\u001b[39mget_representation(chars[\u001b[38;5;241m0\u001b[39m], charoffsets[\u001b[38;5;241m0\u001b[39m], charlens, char_orig_idx)\n\u001b[1;32m    210\u001b[0m char_reps_forward \u001b[38;5;241m=\u001b[39m PackedSequence(char_reps_forward\u001b[38;5;241m.\u001b[39mdata, char_reps_forward\u001b[38;5;241m.\u001b[39mbatch_sizes)\n\u001b[0;32m--> 211\u001b[0m char_reps_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmodel_backward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharoffsets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_orig_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m char_reps_backward \u001b[38;5;241m=\u001b[39m PackedSequence(char_reps_backward\u001b[38;5;241m.\u001b[39mdata, char_reps_backward\u001b[38;5;241m.\u001b[39mbatch_sizes)\n\u001b[1;32m    213\u001b[0m inputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [char_reps_forward, char_reps_backward]\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:160\u001b[0m, in \u001b[0;36mCharacterLanguageModel.get_representation\u001b[0;34m(self, chars, charoffsets, charlens, char_orig_idx)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_representation\u001b[39m(\u001b[38;5;28mself\u001b[39m, chars, charoffsets, charlens, char_orig_idx):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 160\u001b[0m         output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(charoffsets)]\n\u001b[1;32m    162\u001b[0m         res \u001b[38;5;241m=\u001b[39m unsort(res, char_orig_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/rnn.py:920\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    917\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    918\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    923\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#提取成分句法\n",
    "i = 0\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    ll = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            ll.append(line)\n",
    "            doc = nlp(line)\n",
    "        \n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            ll.append(m)\n",
    "            result = build_constituency_feature_tree(m, 2)\n",
    "            ll.append(\"layer 2:\")\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                ll.append(item)\n",
    "                feature_sentences_map[item].append(line)\n",
    "            result = build_constituency_feature_tree(m, 3)\n",
    "            ll.append(\"layer 3:\")\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                ll.append(item)\n",
    "                feature_sentences_map[item].append(line)\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            ll.append(\"Dependency Features:\")\n",
    "            for dep in dependency_list:\n",
    "                ll.append(dep)  # 用制表符拼接特征\n",
    "        \n",
    "            ll.append(\"----------------------------------------------------------------------\")\n",
    "    \n",
    "            with open(f'./tmp/feature{i}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(ll))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这 是 nsubj\n",
      "并 坏事 mark\n",
      "不 坏事 advmod\n",
      "一定 坏事 advmod\n",
      "是 坏事 cop\n",
      "坏事 是 advcl\n",
      "， 坏事 punct\n",
      "尤其 是 advmod\n",
      "是 。 root\n",
      "它 发出 nsubj\n",
      "向 日本 case\n",
      "日本 发出 obl\n",
      "和 韩国 cc\n",
      "韩国 日本 conj\n",
      "这 个 det\n",
      "两 个 nummod\n",
      "个 盟友 nmod\n",
      "盟友 发出 obl\n",
      "发出 是 ccomp\n",
      "了 发出 aux\n",
      "几 个 nummod\n",
      "个 信息 clf\n",
      "至关重要 信息 amod\n",
      "的 至关重要 mark:rel\n",
      "信息 发出 obj\n",
      "。 是 punct\n"
     ]
    }
   ],
   "source": [
    "#lemma是当前token\n",
    "sent = '这并不一定是坏事，尤其是它向日本和韩国这两个盟友发出了几个至关重要的信息。'\n",
    "doc = nlp(sent)\n",
    "# print(doc.sentences[0])\n",
    "for i in doc.sentences[0].words:\n",
    "    head_id = i.head\n",
    "    print(i.lemma, doc.sentences[0].words[head_id-1].lemma, i.deprel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m                 head_word \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mwords[word\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m                 dependency_list\u001b[38;5;241m.\u001b[39mappend((word, head_word, word\u001b[38;5;241m.\u001b[39mdeprel))\n\u001b[0;32m---> 34\u001b[0m             ll\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mextract_dependency_feature_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdependency_list\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     36\u001b[0m         ll\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 输出到文件\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mextract_dependency_feature_from_list\u001b[0;34m(dependency_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     head_word \u001b[38;5;241m=\u001b[39m dep[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     relation \u001b[38;5;241m=\u001b[39m dep[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     head_upos \u001b[38;5;241m=\u001b[39m \u001b[43mhead_word\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupos\u001b[49m\n\u001b[1;32m      9\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mupos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_upos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_word\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mhead_word\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROOT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'upos'"
     ]
    }
   ],
   "source": [
    "# test提取依存树 暂时不用，好像错删了什么\n",
    "def extract_dependency_feature_from_list(dependency_list):\n",
    "    #lemma是当前token\n",
    "    sent = '这并不一定是坏事，尤其是它向日本和韩国这两个盟友发出了几个至关重要的信息。'\n",
    "    doc = nlp(sent)\n",
    "# print(doc.sentences[0])\n",
    "    for i in doc.sentences[0].words:\n",
    "        head_id = i.head\n",
    "        print(i.lemma, doc.sentences[0].words[head_id-1].lemma, i.deprel)\n",
    "    \n",
    "    # result = []\n",
    "    # for dep in dependency_list:\n",
    "    #     word = dep[0]\n",
    "    #     head_word = dep[1]\n",
    "    #     relation = dep[2]\n",
    "    #     head_upos = head_word.upos\n",
    "    #     result.append(f\"{word.upos}/{head_upos}/{relation} {word.text}/{head_word.text if head_word else 'ROOT'}/{relation}\")\n",
    "    # return result\n",
    "\n",
    "# 测试构造 tmp/test_feature.txt\n",
    "ll = []\n",
    "feature_sentences_map = defaultdict(list)\n",
    "with open('./instruct_txt/A01A_en_zh.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        ll.append(line.strip())\n",
    "        doc = nlp(line)\n",
    "        \n",
    "        # Constituency tree\n",
    "        if hasattr(doc.sentences[0], 'constituency'):\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            ll.append(m)\n",
    "        else:\n",
    "            ll.append(\"No constituency analysis available.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        # Dependency tree\n",
    "        # for sentence in doc.sentences:\n",
    "        #     dependency_list = []\n",
    "        #     for word in sentence.words:\n",
    "        #         head_word = sentence.words[word.head - 1] if word.head > 0 else None\n",
    "        #         dependency_list.append((word, head_word, word.deprel))\n",
    "        #     ll.append(\"\\n\".join(extract_dependency_feature_from_list(dependency_list)))\n",
    "\n",
    "        ll.append(\"----------------------------------------------------------------------\")\n",
    "\n",
    "# 输出到文件\n",
    "with open('tmp/test_feature.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = dict(sorted(feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "with open('feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences_data = dict(sorted(feature_sentences_map.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "with open('feature_sentences.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_sentences_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IP': {'IP': {'NP': {}, 'VP': {}}, ',': {}, 'IP83738': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}, '.': {}}}\n",
      "{'IP': {'IP': {'NP': {}, 'VP': {}}, ',': {}, 'IP': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}, '.': {}}}\n",
      "{'IP': {'NP': {}, 'VP': {}}}\n",
      "{'IP': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}}\n"
     ]
    }
   ],
   "source": [
    "text_book = ['(ROOT (IP (IP (NP (NNP 平壤)) (VP (MD 可能) (VP (ADVP (RB 也)) (ADVP (RB 在)) (VP (VV 准备) (IP (VP (VV 试射) (NP (QP (CD 一) (CLP (NNB 枚))) (NP (NP (FW Taepo)) (FW Dong-2)) (NP (NN 导弹))))))))) (, ，) (IP (LCP (NP (NN 理论)) (IN 上)) (, ，) (NP (DP (DT 这) (CLP (NNB 种))) (NP (NN 导弹))) (VP (MD 可以) (VP (VP (VV 携带) (NP (NN 核弹) (SFN 头))) (, ，) (VP (VV 覆盖) (NP (DP (DT 整个)) (NP (NNP 美国)) (NP (NN 大陆))))))) (. 。)))']\n",
    "\n",
    "result = build_constituency_feature_tree(text_book[0], 3)\n",
    "print(result)\n",
    "\n",
    "ff = extract_constituency_feature_from_tree(result)\n",
    "for item in ff:\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN/NUM/nummod', 'VERB/NOUN/nsubj', 'VERB/ADV/mark', 'VERB/ADV/advmod', 'VERB/VERB/advcl', 'VERB/ADV/mark', 'None/VERB/root', 'VERB/AUX/aux', 'VERB/NOUN/nmod:tmod', 'VERB/VERB/xcomp', 'NOUN/NUM/nummod', 'VERB/NOUN/obj', 'VERB/PUNCT/punct']\n"
     ]
    }
   ],
   "source": [
    "print(extract_dependency_feature_from_list(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Secretary Clinton's Asia Trip: Allied Reassurance 希拉里克林顿的亚洲之行：放心吧，盟友\n",
      "Coming only three weeks into the Obama Administration, Secretary of State Hillary Clinton's Asia trip will be long on signals and short on substance. 进入奥巴马内阁仅仅三周，国务卿希拉里克林顿的亚洲之行信号多于实质。\n",
      "That is not necessarily a bad thing, especially when it sends several critically important messages to allies Japan and South Korea. 这并不是坏事，特别是访问向美国的盟友日本和韩国发出了几个极为重要的信息。\n",
      "Her trip communicates that Asia matters to the United States and that Washington is committed to a predominant role in the region over the long-term. 她的访问告诉大家亚洲事务对于美国来说很重要，华盛顿致力于长期在这个区域内扮演一个有影响力的角色。\n",
      "Traveling to Tokyo and Seoul prior to Beijing reflects the importance of our allies as well as a direct attempt to assuage fears of \"Japan passing.\" 在访问北京之前访问东京和首尔反映出了我们的盟友的重要性以及想要缓解“日本已经过气了”的担心的直接尝试。\n",
      "As a Senator, Hillary Clinton authored a Foreign Affairs article in which she stated the U.S.--China relationship was the most important relationship in Asia, rekindling Japanese angst from the slight suffered when President Bill Clinton traveled to China but skipped Japan. 作为参议员，希拉里克林顿在所写的“国际事务”的文章中说到美中关系是亚洲地区最为重要的关系。在比尔克林顿总统当政期间他就访问北京而跳过了日本，从而使得日本小小的伤心了一把，希拉里此文不禁重新点燃了他们的担心。\n",
      "By traveling prior to the Obama Administration having even announced its Asian team or finalized its policies for the region, Clinton says she wants to establish a tone of open dialogue and transparency with America's Asian allies. 在奥巴马政府还没有宣布他的亚洲团队或最终决定对于该地区的政策之前克林顿就开始了访问，她说想要与美国的亚洲盟友之间建立一种公开对话和透明的主基调。\n",
      "But, of course, talking does not guarantee agreement, and there are a number of contentious issues that could arise during the trip. 但是，对话当然不能保证双方达成一致，而且在访问过程中可能有一些有争议的话题出现。\n",
      "Clinton must reassure allies disturbed by Bush Administration's abandonment of important principles in its zeal to achieve North Korean denuclearization. 克林顿必须向为布什政府的放弃态度而烦恼的盟友们保证美国将致力于实现北朝鲜解除核武器的重要原则。\n",
      "Additionally, she should more clearly articulate the Obama Administration's six-party talks strategy and its position on the U.S.--South Korea free trade agreement. 另外，她应该更清楚地说明奥巴马政府对于六方会谈的政策和对美韩自由贸易协定的立场。\n",
      "A Changed Political Landscape 一个已经变化了的政治版图\n",
      "Clinton's trip comes amidst global economic turmoil, which will shift attention away from traditional bilateral and regional security issues, such as transforming the existing military relationships into strategic alliances and achieving North Korean denuclearization. 克林顿的访问在经济危机中进行，这会将注意力从传统的双边和区域安全问题转移开，比如：把已有的军事关系转变为战略伙伴关系和实现北朝鲜无核化等。\n",
      "Moreover, U.S. allies will have fewer resources to address these objectives, including Washington's request for Seoul and Tokyo to assume a larger global security role in Afghanistan. 此外，美国的盟友不会有那么多资源来解决这些问题，包括华盛顿要求首尔和东京在阿富汗担当更大的维护国际安全的角色。\n",
      "Rather than discussing these more traditional concerns, it is expected that Clinton's trip will focus heavily on coordinating policies to address the economic crisis. 相比这些更传统的话题，他们会期望克林顿的这次访问会着重于讨论如何调整策略战胜经济危机。\n",
      "The election of a U.S. President with very high public approval ratings changes the political landscape in bilateral U.S. relations with Tokyo and Seoul as well as domestic South Korean politics. 奥巴马以高支持率当选美国总统改变了美国与韩国，日本双边关系以及韩国国内政策的政治版图。\n",
      "Obama will have more leverage in bilateral relations because he starts with a clean slate. 奥巴马与两国言归于好使得他在双边关系中拥有更有利的位置。\n",
      "For instance, Obama undermines South Korean progressives on the North Korea problem by removing the political cover that their pundits, media, and anti-U.S. groups have hidden behind. 例如，通过掀开韩国学者，媒体和反美团体躲藏的政治屏障，奥巴马破坏了韩国在朝鲜问题上慢慢推进的政策。\n",
      "Anti-U.S. actions and rhetoric can no longer be justified as only being anti-Bush, nor can U.S. efforts to insist upon North Korean denuclearization compliance be blamed on \"neo-conservative conspiracies.\" 反美的言论和行为不能再被正名为仅仅是“反布什”，美国在坚持北朝鲜无核化的努力不能再被指责为“新保守主义阴谋”。\n",
      "The South Korean left will also be less able to blame the deadlocked six-party talks on lack of U.S. \"flexibility,\" i.e., concessions. 韩国左翼也不能轻易的将造成六方会谈的死局的原意指责为美国\"缺乏弹性“，如，让步。\n",
      "It will become increasingly evident that the problem is, as it always has been, North Korean intransigence. 人们会越来越明显的看到，问题一直是北朝鲜的不妥协态度。\n",
      "Clinton's trip occurs when the Japanese and South Korean leaders are weakened by dismally low approval ratings and hamstrung by legislative inertia. 克林顿此次出访正值日本和韩国的领导人处于极低的支持率和被立法程序拖住后腿的状态中，因而两国领导的能量都被削弱。\n",
      "Japan suffers a \"twisted parliament\" with control of the bilateral Diet split between the ruling and opposition parties, the latter of which employs obstructionist tactics to bring about the prime minister's downfall. 日本正处于“绞合的国会”的痛苦中，双边会谈分裂为在朝党和在野党两个，后者使用了障碍策略来促使首相下台。\n",
      "The South Korean ruling Grand National Party is driven by factionalism and acts like a minority party despite having nearly two-thirds control of the unicameral National Assembly. 韩国的在朝党大国家党虽然拥有国民大会几乎三分之二的单院席位，但是受党派之争的印象，大国家党看起来更像一个少数党。\n",
      "Neither Prime Minister Taro Aso nor President Lee Myung-bak have progressed in expanding the military alliances, and both are likely to be resistant to the Obama Administration's requests for military assistance in Afghanistan. 麻生太郎或者李明博都没有在扩大军事联盟上有进展，而且两人都可能对奥巴马政府提出的增兵阿富汗的要求产生抵抗心理。\n",
      "An Increasingly Belligerent North Korea 越来越好战的北朝鲜\n",
      "Clinton's trip will be overshadowed by North Korea's increasing belligerency and potential long-range missile launch. 克林顿此次访问由于北朝鲜的日益好战和可能的远程导弹发射而被蒙上一层阴影。\n",
      "Pyongyang has stepped up its verbal attacks and threatened military action against South Korea in an attempt to get President Lee Myung-bak to abandon his principled policy requiring conditionality, reciprocity, and transparency in inter-Korean relations. 平壤已经开始口头攻击和威胁对韩国使用武力以期能借此使得李明博总统放弃他的韩朝之间有约束的，互利的，透明的关系原则。\n",
      "North Korea's rhetoric is also a shot across the bow of the Obama Administration. 北朝鲜的言论也是对奥巴马政府的一个警告。\n",
      "In mid-January, North Korea rejected the existing six-party agreement by asserting that it would denuclearize only upon receiving formal diplomatic relations with the U.S., the cessation of Washington's \"hostile policy,\" and removal of the U.S. nuclear umbrella over South Korea. 在七月中旬，北朝鲜拒绝了已有的六方会谈协议，声称只有在与美国正式建交，华盛顿停止“不友好的态度”和撤除美国对韩国的核保护伞后才能实现无核化。\n",
      "Pyongyang may also be preparing to test launch a Taepo Dong-2 missile, which could theoretically have the range to reach the continental United States with a nuclear warhead. 平壤可能准备试射射一枚大浦洞2号导弹，该导弹配有核弹头且理论上能够射到大洋彼岸的美国本土。\n",
      "North Korea's actions are a clear signal that it will not adopt a more accommodating position despite the change in U.S. leadership. 北朝鲜的行为明确的表示尽管美国领导人改变了，但它不会采取更合作的态度。\n",
      "Such behavior is a standard North Korean negotiating tactic, designed to raise the ante, deflect criticism of its own noncompliance by blaming U.S. actions, insist on equality of conditions in response to unequal violations, and prompt renegotiation of the existing agreement. 这种行为是标准的北朝鲜谈判策略，以抬高筹码，将对他们不服从的指责转移到美国身上，坚持对不一样的违约采取同样的处理条款，而且鼓动对已有协议的重新谈判。\n",
      "There is, however, a growing risk of a tactical confrontation between the Korean navies in the West Sea that runs the risk of miscalculation and escalation. 然而，韩朝海军之间在西海发生战术对抗的风险可能因为误算或扩张而增加。\n",
      "What Should Be Done 应该做什么\n",
      "During her trip, Clinton must strike a proper balance between accommodating allied concerns while still advocating strong U.S. objectives. 在她的访问中，克林顿必须在顺应同盟的要求和宣传美国的目的之间找到一个合适的平衡。\n",
      "Although Seoul and Tokyo have both agreed to expand the scope of the respective bilateral alliances, implementation has lagged. 尽管首尔和东京都同意扩大各自的双边联盟，但是实际的推行却拖后。\n",
      "Clinton should welcome the improvement in bilateral relations with South Korea as well as the resurgence of military contingency planning that has occurred since the departure of the Roh Moo-hyun administration. 克林顿应该欢迎与韩国之间的双边关系的改进以及卢武玄政府离任时重新开始的应对意外的军队编制。\n",
      "The U.S. should continue to press for an expanded alliance structure with Seoul but temper this effort with a realization of the volatility of the domestic political landscape. 美国应该继续宣传其与首尔扩大的同盟架构但是也应该认识到国内政治格局的不稳定性，因而对这种宣传加以调整。\n",
      "Any Obama Administration request for South Korean ground forces to support coalition operations in Afghanistan will be particularly contentious and would require extensive public diplomacy efforts to mitigate the potential for public demonstrations. 奥巴马政府的任何要求南韩地面部队支持在阿富汗的共同行动的提议都会特别引起争议，而且会需要广泛的外交努力来减轻引发公众示威的可能性。\n",
      "The military relationship with Japan has been problematic due to Japanese constitutional, legal, fiscal, and societal constraints as well as extensive foot-dragging by Tokyo. 与日本的军事关系问题重重，原因是日本的宪法，法律，财政和社会上的限制以及东京的迟疑不决。\n",
      "It is now unclear the degree to which Japan is willing to alter the comfortable alliance status quo. 现在还不清楚日本愿意改变目前舒适的联盟状态的程度。\n",
      "There are serious consequences to Japanese inaction. 日本的无为会有严重的后果。\n",
      "Long-term Japanese policy stagnation is not in Washington's strategic interest and risks increasing U.S. frustration with its ally. 日本长期政策的停滞不前不符合华盛顿的战略利益，而且这样一来，美国的盟友对其越来越失望的风险会增大。\n",
      "Tokyo's unwillingness or inability to make tough decisions risks Japan losing influence and even relevance in a region increasingly dominated by an ascendant China. 东京不愿意或者不能够做强硬的决定是冒着失去在地区中的影响力甚至关联性的风险的，在区域中上升的中国越来越处于主导位置。\n",
      "Clinton should articulate U.S. concerns while exploring Japanese objectives in preparation for subsequent efforts coordinated with the Secretary of Defense. 克林顿应该清楚地说明美国的担忧同时试探日本的目的，为下一步由国防部长协调的努力做准备。\n",
      "Clinton should reassure our allies that the U.S. remains committed to the complete and verifiable denuclearization of North Korea and unequivocally state that Washington will not accept North Korea as a nuclear weapons state. 克林顿应该向盟友们重新确认美国仍然致力于北朝鲜的完全和可证实的无核化以及华盛顿不会接受北朝鲜作为核武器持有国的清晰的态度。\n",
      "Statements by U.S. officials have been misperceived in South Korea and Japan as indicating a shift in U.S. policy away from North Korean denuclearization to merely capping the existing nuclear stockpile and preventing proliferation. 美国官方的声明被南韩和日本错误理解为美国对北朝鲜无核化的政策转为只要停止已有的核储存以及组织增殖。\n",
      "Clinton should also delineate the Obama Administration's strategy toward the six-party talks and the means it will employ to ensure North Korean compliance with existing commitments. 克林顿也应该说明奥巴马政府对于六方会谈的政策以及美国为了确认北朝鲜实现已有的承诺所使用的方法。\n"
     ]
    }
   ],
   "source": [
    "for zh, en in parse_tmx('./Yiyan_tmx/A01A.tmx'):\n",
    "    print(zh, en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
