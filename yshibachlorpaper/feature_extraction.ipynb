{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 13:28:38 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b1956b32df4671a4e815f43148bb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-19 13:28:48 INFO: Downloaded file to /Users/yishi/stanza_resources/resources.json\n",
      "2025-01-19 13:28:48 INFO: \"zh\" is an alias for \"zh-hans\"\n",
      "2025-01-19 13:28:50 INFO: Loading these models for language: zh-hans (Simplified_Chinese):\n",
      "===================================\n",
      "| Processor    | Package          |\n",
      "-----------------------------------\n",
      "| tokenize     | gsdsimp          |\n",
      "| pos          | gsdsimp_charlm   |\n",
      "| lemma        | gsdsimp_nocharlm |\n",
      "| constituency | ctb-51_charlm    |\n",
      "| depparse     | gsdsimp_charlm   |\n",
      "| sentiment    | ren_charlm       |\n",
      "| ner          | ontonotes        |\n",
      "===================================\n",
      "\n",
      "2025-01-19 13:28:50 INFO: Using device: cpu\n",
      "2025-01-19 13:28:50 INFO: Loading: tokenize\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:50 INFO: Loading: pos\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:51 INFO: Loading: lemma\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/lemma/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:51 INFO: Loading: constituency\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/constituency/trainer.py:236: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:51 INFO: Loading: depparse\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/depparse/trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:52 INFO: Loading: sentiment\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/classifiers/trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:52 INFO: Loading: ner\n",
      "/Users/yishi/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/ner/trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2025-01-19 13:28:54 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# stanza pipeline for constituency and dependency parsing\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions and data structures for feature extraction\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "#构造嵌套字典\n",
    "def tree():\n",
    "    return defaultdict(tree)\n",
    "#去掉空格\n",
    "def split_string(text):\n",
    "    result = []\n",
    "    tmp = \"\"\n",
    "    flag = 0\n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            flag = 0\n",
    "            if tmp != \"\":\n",
    "                result.append(tmp)\n",
    "                tmp = \"\"\n",
    "            continue\n",
    "        elif char == '(':\n",
    "            flag = 1\n",
    "            result.append(char)\n",
    "            continue\n",
    "        elif char == ')':\n",
    "            result.append(char)\n",
    "            continue\n",
    "        elif flag == 1:\n",
    "            tmp += char\n",
    "            continue\n",
    "    return result, ''.join(result)\n",
    "\n",
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def is_empty(self):\n",
    "        return self.items == []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items.pop()\n",
    "        else:\n",
    "            print(\"Stack is empty\")\n",
    "\n",
    "    def top(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items[-1]\n",
    "        else:\n",
    "            print(\"Stack is empty\")\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.items)\n",
    "\n",
    "#将递归的 defaultdict 转换为普通的嵌套字典。\n",
    "def dicts(t): return {k: dicts(t[k]) for k in t}\n",
    "\n",
    "def build_constituency_feature_tree(text, MAXLAYER=3):\n",
    "    treeStrList, tmp = split_string(text)\n",
    "    featureTree = tree()\n",
    "    stack = Stack()\n",
    "    i = 0\n",
    "    layer = -1\n",
    "    while i < len(treeStrList):\n",
    "        if treeStrList[i] == '(':\n",
    "            i+=1\n",
    "            layer+=1\n",
    "            if layer <= MAXLAYER:\n",
    "                if(stack.is_empty()):\n",
    "                    featureTree[treeStrList[i]]\n",
    "                    stack.push(featureTree[treeStrList[i]])\n",
    "                else:\n",
    "                    if(treeStrList[i] in stack.top()):\n",
    "                        keytmp = treeStrList[i] + str(random.randint(1, 100000)) #发现重复键，则附加随机数\n",
    "                    else:\n",
    "                        keytmp = treeStrList[i]\n",
    "                    stack.top()[keytmp]\n",
    "                    stack.push(stack.top()[keytmp])\n",
    "            i+=1\n",
    "            continue\n",
    "        if treeStrList[i] == ')':\n",
    "            if layer > 0 and layer <= MAXLAYER:\n",
    "                stack.pop()\n",
    "            layer-=1\n",
    "            i+=1\n",
    "            continue\n",
    "    return dicts(featureTree)['ROOT']\n",
    "\n",
    "def extract_constituency_feature_from_tree(feature_tree):\n",
    "    queue = []\n",
    "    result = []\n",
    "    queue.append(feature_tree)\n",
    "    while len(queue) > 0:\n",
    "        tmp = queue.pop()\n",
    "        for key in tmp.keys():\n",
    "            if isinstance(tmp[key], dict) and len(tmp[key]) > 0:\n",
    "                queue.append(tmp[key])\n",
    "                # 去除数字\n",
    "                result.append(''.join(char for char in str({key: tmp[key]}) if not char.isdigit()))\n",
    "    return result\n",
    "\n",
    "# zcy fix return type, Time: 2025.01.16\n",
    "def extract_dependency_feature_from_list(words):\n",
    "    sent_list = []\n",
    "    for word in words:\n",
    "        head_id = word.head\n",
    "        head_upos = \"ROOT\" if head_id == 0 else words[head_id - 1].upos  # 获取 head 的 UPOS\n",
    "        pos_list = [word.upos, head_upos, word.deprel]\n",
    "        head_lemma = \"NONE\" if head_id == 0 else words[head_id - 1].lemma\n",
    "        lex_list = [word.lemma, head_lemma, word.deprel]\n",
    "        sent_list.append([f'{pos_list}', f'{lex_list}'])\n",
    "    return sent_list\n",
    "\n",
    "def parse_tmx(file):\n",
    "    # 解析 XML 文件\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 找到 TMX 文件中的 body 部分\n",
    "    body = root.find(\"body\")\n",
    "    if body is None:\n",
    "        raise ValueError(\"Invalid TMX file: missing <body> section.\")\n",
    "\n",
    "    # 遍历所有 <tu> 元素\n",
    "    for tu in body.findall(\"tu\"):\n",
    "        translations = {}\n",
    "        # 提取每个 <tu> 中的 <tuv> 元素\n",
    "        for tuv in tu.findall(\"tuv\"):\n",
    "            lang = tuv.attrib.get(\"{http://www.w3.org/XML/1998/namespace}lang\")  # 获取语言属性\n",
    "            seg = tuv.find(\"seg\")\n",
    "            if lang and seg is not None:\n",
    "                translations[lang] = seg.text.strip()\n",
    "\n",
    "        # 如果有源语言和目标语言，返回一对\n",
    "        if \"en-US\" in translations and \"zh-CN\" in translations:\n",
    "            yield translations[\"en-US\"], translations[\"zh-CN\"]\n",
    "\n",
    "def list_txt_files(folder_path):\n",
    "    result = []\n",
    "    # 遍历文件夹及其子文件夹中的所有文件\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):  # 只处理以 .txt 结尾的文件\n",
    "                file_path = os.path.join(root, file)\n",
    "                result.append(file_path)  # 返回文件的完整路径\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  33%|███▎      | 1/3 [13:53<27:47, 833.89s/file]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Constituency\u001b[39;00m\n\u001b[1;32m     12\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(doc\u001b[38;5;241m.\u001b[39msentences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconstituency)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/sentiment_processor.py:66\u001b[0m, in \u001b[0;36mSentimentProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     64\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mextract_sentences(document)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 66\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# TODO: allow a classifier processor for any attribute, not just sentiment\u001b[39;00m\n\u001b[1;32m     68\u001b[0m document\u001b[38;5;241m.\u001b[39mset(SENTIMENT, labels, to_sentence\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/classifiers/base_classifier.py:53\u001b[0m, in \u001b[0;36mBaseClassifier.label_sentences\u001b[0;34m(self, sentences, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interval[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m interval[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# this can happen for empty text\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m[\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m labels\u001b[38;5;241m.\u001b[39mextend(predicted\u001b[38;5;241m.\u001b[39mtolist())\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/classifiers/cnn_classifier.py:461\u001b[0m, in \u001b[0;36mCNNClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m     all_inputs\u001b[38;5;241m.\u001b[39mappend(char_reps_forward)\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward_charlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m     char_reps_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_reps\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_phrase_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_charlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmodel_backward_projection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbegin_paddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m     all_inputs\u001b[38;5;241m.\u001b[39mappend(char_reps_backward)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_elmo:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# this will be N arrays of 3xMx1024 where M is the number of words\u001b[39;00m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# and N is the number of sentences (and 1024 is actually the number of weights)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/classifiers/cnn_classifier.py:313\u001b[0m, in \u001b[0;36mCNNClassifier.build_char_reps\u001b[0;34m(self, inputs, max_phrase_len, charlm, projection, begin_paddings, device)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_char_reps\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, max_phrase_len, charlm, projection, begin_paddings, device):\n\u001b[0;32m--> 313\u001b[0m     char_reps \u001b[38;5;241m=\u001b[39m \u001b[43mcharlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m projection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m         char_reps \u001b[38;5;241m=\u001b[39m [projection(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m char_reps]\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:222\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    219\u001b[0m chars \u001b[38;5;241m=\u001b[39m get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id\u001b[38;5;241m=\u001b[39mvocab\u001b[38;5;241m.\u001b[39munit2id(CHARLM_END))\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 222\u001b[0m     output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[1;32m    224\u001b[0m     res \u001b[38;5;241m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/rnn.py:920\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    917\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    918\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    923\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#combined, calculate dep and const feature\n",
    "#sy added 1.19\n",
    "Constituency_feature_count_map = defaultdict(int)\n",
    "Dependency_feature_count_map = defaultdict(int)\n",
    "for file in tqdm(list_txt_files(\"./LCMC_txt\"), total=3, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            doc = nlp(line)\n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            layer_2_feature_tree = build_constituency_feature_tree(m, 2)\n",
    "            sec_feature_list = extract_constituency_feature_from_tree(layer_2_feature_tree)\n",
    "            for i in sec_feature_list:\n",
    "                Constituency_feature_count_map[i] += 1\n",
    "            \n",
    "            layer_3_feature_tree = build_constituency_feature_tree(m, 3)\n",
    "            sec_feature_list = extract_constituency_feature_from_tree(layer_3_feature_tree)\n",
    "            for i in sec_feature_list:\n",
    "                Constituency_feature_count_map[i] += 1\n",
    "           \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                Dependency_feature_count_map[dep[0]] += 1\n",
    "\n",
    "for file in tqdm(list_txt_files(\"./translated_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            doc = nlp(line)\n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            layer_2_feature_tree = build_constituency_feature_tree(m, 2)\n",
    "            sec_feature_list = extract_constituency_feature_from_tree(layer_2_feature_tree)\n",
    "            for i in sec_feature_list:\n",
    "                Constituency_feature_count_map[i] += 1\n",
    "            \n",
    "            layer_3_feature_tree = build_constituency_feature_tree(m, 3)\n",
    "            sec_feature_list = extract_constituency_feature_from_tree(layer_3_feature_tree)\n",
    "            for i in sec_feature_list:\n",
    "                Constituency_feature_count_map[i] += 1\n",
    "           \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                Dependency_feature_count_map[dep[0]] += 1\n",
    "\n",
    "sorted_const_data = dict(sorted(Constituency_feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "sorted_dep_data = dict(sorted(Dependency_feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(f'./combined_constituency_feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_const_data, f, ensure_ascii=False, indent=4)\n",
    "with open(f'./combined_dependency_feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_dep_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index\n",
    "with open('./all_feature.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#TODO: 检查data是否是字典\n",
    "\n",
    "# 遍历字典，将值改为递增的索引\n",
    "updated_data = {}\n",
    "for index, key in enumerate(data.keys()):\n",
    "    updated_data[key] = index\n",
    "\n",
    "# 写入新的 JSON 文件\n",
    "with open('./all_feature_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(updated_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count_map = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sentences_map = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zcy add Time: 2025.01.16\n",
    "# 读取 JSON 文件\n",
    "with open('./all_feature.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#TODO: 检查data是否是字典\n",
    "\n",
    "# 遍历字典，将值改为递增的索引\n",
    "updated_data = {}\n",
    "for index, key in enumerate(data.keys()):\n",
    "    updated_data[key] = index\n",
    "\n",
    "# 写入新的 JSON 文件\n",
    "with open('./all_feature_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(updated_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 88/88 [10:40<00:00,  7.28s/file]\n"
     ]
    }
   ],
   "source": [
    "#ed\n",
    "\n",
    "Dependency_feature_count_map = defaultdict(int)\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            doc = nlp(line)\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                Dependency_feature_count_map[dep[0]] += 1\n",
    "sorted_data = dict(sorted(Dependency_feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(f'./dependency_feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3/3 [1:38:07<00:00, 1962.37s/file]\n"
     ]
    }
   ],
   "source": [
    "#orig building tmp\n",
    "i = 0\n",
    "for file in tqdm(list_txt_files(\"./LCMC_txt\"), total=3, desc=\"Processing\", unit=\"file\"):\n",
    "    ll = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line != '\\n':\n",
    "                ll.append(line)\n",
    "                doc = nlp(line)\n",
    "        \n",
    "                # Constituency\n",
    "                m = str(doc.sentences[0].constituency)\n",
    "                ll.append(m)\n",
    "                result = build_constituency_feature_tree(m, 2)\n",
    "                ll.append(\"layer 2:\")\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    ll.append(item)\n",
    "                    feature_sentences_map[item].append(line)\n",
    "                result = build_constituency_feature_tree(m, 3)\n",
    "                ll.append(\"layer 3:\")\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    ll.append(item)\n",
    "                    feature_sentences_map[item].append(line)\n",
    "            \n",
    "                # Dependency\n",
    "                words = doc.sentences[0].words\n",
    "                dependency_list = extract_dependency_feature_from_list(words)\n",
    "                ll.append(\"Dependency Features:\")\n",
    "                for dep in dependency_list:\n",
    "                    ll.append(dep[0])  # 用制表符拼接特征\n",
    "            \n",
    "                ll.append(\"----------------------------------------------------------------------\")\n",
    "        \n",
    "                with open(f'./orig_tmp/feature{i}.txt', 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(ll))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index orig feature \n",
    "#sy\n",
    "with open('./all_feature.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#TODO: 检查data是否是字典\n",
    "\n",
    "# 遍历字典，将值改为递增的索引\n",
    "updated_data = {}\n",
    "for index, key in enumerate(data.keys()):\n",
    "    updated_data[key] = index\n",
    "\n",
    "# 写入新的 JSON 文件\n",
    "with open('./all_feature_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(updated_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 88/88 [11:11<00:00,  7.63s/file]\n"
     ]
    }
   ],
   "source": [
    "# zcy add Time: 2025.01.16\n",
    "\n",
    "with open('./all_feature_index.json', 'r', encoding='utf-8') as f:\n",
    "    updated_data = json.load(f)\n",
    "result_vec = []\n",
    "dic_re = []\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            doc = nlp(line)\n",
    "            l1 = [0] * 629\n",
    "            dict_ft_txt = {}\n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            result = build_constituency_feature_tree(m, 2)\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                if item in updated_data:\n",
    "                    l1[updated_data[item]] += 1\n",
    "                    dict_ft_txt[updated_data[item]] = item\n",
    "            result = build_constituency_feature_tree(m, 3)\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                if item in updated_data:\n",
    "                    l1[updated_data[item]] += 1\n",
    "                    dict_ft_txt[updated_data[item]] = item\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                if dep[0] in updated_data:\n",
    "                    l1[updated_data[dep[0]]] += 1\n",
    "                    dict_ft_txt[updated_data[dep[0]]] = dep[1]\n",
    "            dic_re.append(dict_ft_txt)\n",
    "            result_vec.append(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig build vec\n",
    "\n",
    "with open('./all_feature_index.json', 'r', encoding='utf-8') as f:\n",
    "    updated_data = json.load(f)\n",
    "result_vec = []\n",
    "dic_re = []\n",
    "for file in tqdm(list_txt_files(\"./LCMC_txt\"), total=3, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line == \"\":\n",
    "                doc = nlp(line)\n",
    "                l1 = [0] * 629\n",
    "                dict_ft_txt = {}\n",
    "                # Constituency\n",
    "                m = str(doc.sentences[0].constituency)\n",
    "                result = build_constituency_feature_tree(m, 2)\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    if item in updated_data:\n",
    "                        l1[updated_data[item]] += 1\n",
    "                        dict_ft_txt[updated_data[item]] = item\n",
    "                result = build_constituency_feature_tree(m, 3)\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    if item in updated_data:\n",
    "                        l1[updated_data[item]] += 1\n",
    "                        dict_ft_txt[updated_data[item]] = item\n",
    "            \n",
    "                # Dependency\n",
    "                words = doc.sentences[0].words\n",
    "                dependency_list = extract_dependency_feature_from_list(words)\n",
    "                for dep in dependency_list:\n",
    "                    if dep[0] in updated_data:\n",
    "                        l1[updated_data[dep[0]]] += 1\n",
    "                        dict_ft_txt[updated_data[dep[0]]] = dep[1]\n",
    "                dic_re.append(dict_ft_txt)\n",
    "                result_vec.append(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zcy add Time: 2025.01.17\n",
    "with open('all_line_feature.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dic_re, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zcy add Time: 2025.01.17\n",
    "with open(\"./all_line_vec.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for row in result_vec:\n",
    "        file.write(str(row) + \"\\n\")  # 将每行写入，并添加换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这 是 nsubj\n",
      "并 坏事 mark\n",
      "不 坏事 advmod\n",
      "一定 坏事 advmod\n",
      "是 坏事 cop\n",
      "坏事 是 advcl\n",
      "， 坏事 punct\n",
      "尤其 是 advmod\n",
      "是 。 root\n",
      "它 发出 nsubj\n",
      "向 日本 case\n",
      "日本 发出 obl\n",
      "和 韩国 cc\n",
      "韩国 日本 conj\n",
      "这 个 det\n",
      "两 个 nummod\n",
      "个 盟友 nmod\n",
      "盟友 发出 obl\n",
      "发出 是 ccomp\n",
      "了 发出 aux\n",
      "几 个 nummod\n",
      "个 信息 clf\n",
      "至关重要 信息 amod\n",
      "的 至关重要 mark:rel\n",
      "信息 发出 obj\n",
      "。 是 punct\n"
     ]
    }
   ],
   "source": [
    "#lemma是当前token\n",
    "sent = '这并不一定是坏事，尤其是它向日本和韩国这两个盟友发出了几个至关重要的信息。'\n",
    "doc = nlp(sent)\n",
    "# print(doc.sentences[0])\n",
    "for i in doc.sentences[0].words:\n",
    "    head_id = i.head\n",
    "    print(i.lemma, doc.sentences[0].words[head_id-1].lemma, i.deprel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m                 head_word \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mwords[word\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m                 dependency_list\u001b[38;5;241m.\u001b[39mappend((word, head_word, word\u001b[38;5;241m.\u001b[39mdeprel))\n\u001b[0;32m---> 34\u001b[0m             ll\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mextract_dependency_feature_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdependency_list\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     36\u001b[0m         ll\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 输出到文件\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mextract_dependency_feature_from_list\u001b[0;34m(dependency_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     head_word \u001b[38;5;241m=\u001b[39m dep[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     relation \u001b[38;5;241m=\u001b[39m dep[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     head_upos \u001b[38;5;241m=\u001b[39m \u001b[43mhead_word\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupos\u001b[49m\n\u001b[1;32m      9\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mupos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_upos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_word\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mhead_word\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROOT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'upos'"
     ]
    }
   ],
   "source": [
    "# test提取依存树 暂时不用，好像错删了什么\n",
    "def extract_dependency_feature_from_list(dependency_list):\n",
    "    #lemma是当前token\n",
    "    sent = '这并不一定是坏事，尤其是它向日本和韩国这两个盟友发出了几个至关重要的信息。'\n",
    "    doc = nlp(sent)\n",
    "# print(doc.sentences[0])\n",
    "    for i in doc.sentences[0].words:\n",
    "        head_id = i.head\n",
    "        print(i.lemma, doc.sentences[0].words[head_id-1].lemma, i.deprel)\n",
    "    \n",
    "    # result = []\n",
    "    # for dep in dependency_list:\n",
    "    #     word = dep[0]\n",
    "    #     head_word = dep[1]\n",
    "    #     relation = dep[2]\n",
    "    #     head_upos = head_word.upos\n",
    "    #     result.append(f\"{word.upos}/{head_upos}/{relation} {word.text}/{head_word.text if head_word else 'ROOT'}/{relation}\")\n",
    "    # return result\n",
    "\n",
    "# 测试构造 tmp/test_feature.txt\n",
    "ll = []\n",
    "feature_sentences_map = defaultdict(list)\n",
    "with open('./instruct_txt/A01A_en_zh.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        ll.append(line.strip())\n",
    "        doc = nlp(line)\n",
    "        \n",
    "        # Constituency tree\n",
    "        if hasattr(doc.sentences[0], 'constituency'):\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            ll.append(m)\n",
    "        else:\n",
    "            ll.append(\"No constituency analysis available.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        # Dependency tree\n",
    "        # for sentence in doc.sentences:\n",
    "        #     dependency_list = []\n",
    "        #     for word in sentence.words:\n",
    "        #         head_word = sentence.words[word.head - 1] if word.head > 0 else None\n",
    "        #         dependency_list.append((word, head_word, word.deprel))\n",
    "        #     ll.append(\"\\n\".join(extract_dependency_feature_from_list(dependency_list)))\n",
    "\n",
    "        ll.append(\"----------------------------------------------------------------------\")\n",
    "\n",
    "# 输出到文件\n",
    "with open('tmp/test_feature.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = dict(sorted(feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "with open('feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences_data = dict(sorted(feature_sentences_map.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "with open('feature_sentences.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_sentences_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IP': {'IP': {'NP': {}, 'VP': {}}, ',': {}, 'IP83738': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}, '.': {}}}\n",
      "{'IP': {'IP': {'NP': {}, 'VP': {}}, ',': {}, 'IP': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}, '.': {}}}\n",
      "{'IP': {'NP': {}, 'VP': {}}}\n",
      "{'IP': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}}\n"
     ]
    }
   ],
   "source": [
    "text_book = ['(ROOT (IP (IP (NP (NNP 平壤)) (VP (MD 可能) (VP (ADVP (RB 也)) (ADVP (RB 在)) (VP (VV 准备) (IP (VP (VV 试射) (NP (QP (CD 一) (CLP (NNB 枚))) (NP (NP (FW Taepo)) (FW Dong-2)) (NP (NN 导弹))))))))) (, ，) (IP (LCP (NP (NN 理论)) (IN 上)) (, ，) (NP (DP (DT 这) (CLP (NNB 种))) (NP (NN 导弹))) (VP (MD 可以) (VP (VP (VV 携带) (NP (NN 核弹) (SFN 头))) (, ，) (VP (VV 覆盖) (NP (DP (DT 整个)) (NP (NNP 美国)) (NP (NN 大陆))))))) (. 。)))']\n",
    "\n",
    "result = build_constituency_feature_tree(text_book[0], 3)\n",
    "print(result)\n",
    "\n",
    "ff = extract_constituency_feature_from_tree(result)\n",
    "for item in ff:\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN/NUM/nummod', 'VERB/NOUN/nsubj', 'VERB/ADV/mark', 'VERB/ADV/advmod', 'VERB/VERB/advcl', 'VERB/ADV/mark', 'None/VERB/root', 'VERB/AUX/aux', 'VERB/NOUN/nmod:tmod', 'VERB/VERB/xcomp', 'NOUN/NUM/nummod', 'VERB/NOUN/obj', 'VERB/PUNCT/punct']\n"
     ]
    }
   ],
   "source": [
    "print(extract_dependency_feature_from_list(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruct提取成分句法\n",
    "# ed\n",
    "i = 0\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    ll = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            ll.append(line)\n",
    "            doc = nlp(line)\n",
    "        \n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            ll.append(m)\n",
    "            result = build_constituency_feature_tree(m, 2)\n",
    "            ll.append(\"layer 2:\")\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                ll.append(item)\n",
    "                feature_sentences_map[item].append(line)\n",
    "            result = build_constituency_feature_tree(m, 3)\n",
    "            ll.append(\"layer 3:\")\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                ll.append(item)\n",
    "                feature_sentences_map[item].append(line)\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            ll.append(\"Dependency Features:\")\n",
    "            for dep in dependency_list:\n",
    "                ll.append(dep)  # 用制表符拼接特征\n",
    "        \n",
    "            ll.append(\"----------------------------------------------------------------------\")\n",
    "    \n",
    "            with open(f'./tmp/feature{i}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(ll))\n",
    "        i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
