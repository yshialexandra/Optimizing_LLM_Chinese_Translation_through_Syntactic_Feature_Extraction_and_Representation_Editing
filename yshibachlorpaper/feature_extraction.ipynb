{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions and data structures for feature extraction\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "#构造嵌套字典\n",
    "def tree():\n",
    "    return defaultdict(tree)\n",
    "#去掉空格\n",
    "def split_string(text):\n",
    "    result = []\n",
    "    tmp = \"\"\n",
    "    flag = 0\n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            flag = 0\n",
    "            if tmp != \"\":\n",
    "                result.append(tmp)\n",
    "                tmp = \"\"\n",
    "            continue\n",
    "        elif char == '(':\n",
    "            flag = 1\n",
    "            result.append(char)\n",
    "            continue\n",
    "        elif char == ')':\n",
    "            result.append(char)\n",
    "            continue\n",
    "        elif flag == 1:\n",
    "            tmp += char\n",
    "            continue\n",
    "    return result, ''.join(result)\n",
    "\n",
    "class Stack:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def is_empty(self):\n",
    "        return self.items == []\n",
    "\n",
    "    def push(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items.pop()\n",
    "        else:\n",
    "            print(\"Stack is empty\")\n",
    "\n",
    "    def top(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items[-1]\n",
    "        else:\n",
    "            print(\"Stack is empty\")\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.items)\n",
    "\n",
    "#将递归的 defaultdict 转换为普通的嵌套字典。\n",
    "def dicts(t): return {k: dicts(t[k]) for k in t}\n",
    "\n",
    "def build_constituency_feature_tree(text, MAXLAYER=3):\n",
    "    treeStrList, tmp = split_string(text)\n",
    "    featureTree = tree()\n",
    "    stack = Stack()\n",
    "    i = 0\n",
    "    layer = -1\n",
    "    while i < len(treeStrList):\n",
    "        if treeStrList[i] == '(':\n",
    "            i+=1\n",
    "            layer+=1\n",
    "            if layer <= MAXLAYER:\n",
    "                if(stack.is_empty()):\n",
    "                    featureTree[treeStrList[i]]\n",
    "                    stack.push(featureTree[treeStrList[i]])\n",
    "                else:\n",
    "                    if(treeStrList[i] in stack.top()):\n",
    "                        keytmp = treeStrList[i] + str(random.randint(1, 100000)) #发现重复键，则附加随机数\n",
    "                    else:\n",
    "                        keytmp = treeStrList[i]\n",
    "                    stack.top()[keytmp]\n",
    "                    stack.push(stack.top()[keytmp])\n",
    "            i+=1\n",
    "            continue\n",
    "        if treeStrList[i] == ')':\n",
    "            if layer > 0 and layer <= MAXLAYER:\n",
    "                stack.pop()\n",
    "            layer-=1\n",
    "            i+=1\n",
    "            continue\n",
    "    return dicts(featureTree)['ROOT']\n",
    "\n",
    "def extract_constituency_feature_from_tree(feature_tree):\n",
    "    queue = []\n",
    "    result = []\n",
    "    queue.append(feature_tree)\n",
    "    while len(queue) > 0:\n",
    "        tmp = queue.pop()\n",
    "        for key in tmp.keys():\n",
    "            if isinstance(tmp[key], dict) and len(tmp[key]) > 0:\n",
    "                queue.append(tmp[key])\n",
    "                # 去除数字\n",
    "                result.append(''.join(char for char in str({key: tmp[key]}) if not char.isdigit()))\n",
    "    return result\n",
    "\n",
    "# zcy fix return type, Time: 2025.01.16\n",
    "def extract_dependency_feature_from_list(words):\n",
    "    sent_list = []\n",
    "    for word in words:\n",
    "        head_id = word.head\n",
    "        head_upos = \"ROOT\" if head_id == 0 else words[head_id - 1].upos  # 获取 head 的 UPOS\n",
    "        pos_list = [word.upos, head_upos, word.deprel]\n",
    "        head_lemma = \"NONE\" if head_id == 0 else words[head_id - 1].lemma\n",
    "        lex_list = [word.lemma, head_lemma, word.deprel]\n",
    "        sent_list.append([f'{pos_list}', f'{lex_list}'])\n",
    "    return sent_list\n",
    "\n",
    "def parse_tmx(file):\n",
    "    # 解析 XML 文件\n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 找到 TMX 文件中的 body 部分\n",
    "    body = root.find(\"body\")\n",
    "    if body is None:\n",
    "        raise ValueError(\"Invalid TMX file: missing <body> section.\")\n",
    "\n",
    "    # 遍历所有 <tu> 元素\n",
    "    for tu in body.findall(\"tu\"):\n",
    "        translations = {}\n",
    "        # 提取每个 <tu> 中的 <tuv> 元素\n",
    "        for tuv in tu.findall(\"tuv\"):\n",
    "            lang = tuv.attrib.get(\"{http://www.w3.org/XML/1998/namespace}lang\")  # 获取语言属性\n",
    "            seg = tuv.find(\"seg\")\n",
    "            if lang and seg is not None:\n",
    "                translations[lang] = seg.text.strip()\n",
    "\n",
    "        # 如果有源语言和目标语言，返回一对\n",
    "        if \"en-US\" in translations and \"zh-CN\" in translations:\n",
    "            yield translations[\"en-US\"], translations[\"zh-CN\"]\n",
    "\n",
    "def list_txt_files(folder_path):\n",
    "    result = []\n",
    "    # 遍历文件夹及其子文件夹中的所有文件\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):  # 只处理以 .txt 结尾的文件\n",
    "                file_path = os.path.join(root, file)\n",
    "                result.append(file_path)  # 返回文件的完整路径\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza pipeline for constituency and dependency parsing\n",
    "import stanza\n",
    "nlp = stanza.Pipeline('zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_count_map = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sentences_map = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruct提取成分句法\n",
    "# ed\n",
    "i = 0\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    ll = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            ll.append(line)\n",
    "            doc = nlp(line)\n",
    "        \n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            ll.append(m)\n",
    "            result = build_constituency_feature_tree(m, 2)\n",
    "            ll.append(\"layer 2:\")\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                ll.append(item)\n",
    "                feature_sentences_map[item].append(line)\n",
    "            result = build_constituency_feature_tree(m, 3)\n",
    "            ll.append(\"layer 3:\")\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                ll.append(item)\n",
    "                feature_sentences_map[item].append(line)\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            ll.append(\"Dependency Features:\")\n",
    "            for dep in dependency_list:\n",
    "                ll.append(dep)  # 用制表符拼接特征\n",
    "        \n",
    "            ll.append(\"----------------------------------------------------------------------\")\n",
    "    \n",
    "            with open(f'./tmp/feature{i}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(ll))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 3/3 [1:38:07<00:00, 1962.37s/file]\n"
     ]
    }
   ],
   "source": [
    "#orig building tmp\n",
    "i = 0\n",
    "for file in tqdm(list_txt_files(\"./LCMC_txt\"), total=3, desc=\"Processing\", unit=\"file\"):\n",
    "    ll = []\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line != '\\n':\n",
    "                ll.append(line)\n",
    "                doc = nlp(line)\n",
    "        \n",
    "                # Constituency\n",
    "                m = str(doc.sentences[0].constituency)\n",
    "                ll.append(m)\n",
    "                result = build_constituency_feature_tree(m, 2)\n",
    "                ll.append(\"layer 2:\")\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    ll.append(item)\n",
    "                    feature_sentences_map[item].append(line)\n",
    "                result = build_constituency_feature_tree(m, 3)\n",
    "                ll.append(\"layer 3:\")\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    ll.append(item)\n",
    "                    feature_sentences_map[item].append(line)\n",
    "            \n",
    "                # Dependency\n",
    "                words = doc.sentences[0].words\n",
    "                dependency_list = extract_dependency_feature_from_list(words)\n",
    "                ll.append(\"Dependency Features:\")\n",
    "                for dep in dependency_list:\n",
    "                    ll.append(dep[0])  # 用制表符拼接特征\n",
    "            \n",
    "                ll.append(\"----------------------------------------------------------------------\")\n",
    "        \n",
    "                with open(f'./orig_tmp/feature{i}.txt', 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(ll))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 88/88 [10:40<00:00,  7.28s/file]\n"
     ]
    }
   ],
   "source": [
    "#提取成分句法\n",
    "# zcy add Time: 2025.01.16\n",
    "\n",
    "Dependency_feature_count_map = defaultdict(int)\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            doc = nlp(line)\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                Dependency_feature_count_map[dep[0]] += 1\n",
    "sorted_data = dict(sorted(Dependency_feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(f'./dependency_feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   1%|          | 1/88 [35:47<51:54:25, 2147.89s/file]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Constituency\u001b[39;00m\n\u001b[1;32m     12\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(doc\u001b[38;5;241m.\u001b[39msentences[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconstituency)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/pipeline/pos_processor.py:88\u001b[0m, in \u001b[0;36mPOSProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m     87\u001b[0m             idx\u001b[38;5;241m.\u001b[39mextend(b[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 88\u001b[0m             preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m preds \u001b[38;5;241m=\u001b[39m unsort(preds, idx)\n\u001b[1;32m     91\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdoc\u001b[38;5;241m.\u001b[39mset([doc\u001b[38;5;241m.\u001b[39mUPOS, doc\u001b[38;5;241m.\u001b[39mXPOS, doc\u001b[38;5;241m.\u001b[39mFEATS], [y \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m preds \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m x])\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/pos/trainer.py:98\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, unsort)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     97\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_orig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m upos_seqs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m preds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()]\n\u001b[1;32m    100\u001b[0m xpos_seqs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpos\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap(sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m preds[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()]\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/pos/model.py:169\u001b[0m, in \u001b[0;36mTagger.forward\u001b[0;34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\u001b[0m\n\u001b[1;32m    166\u001b[0m     all_forward_chars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_forward_transform(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_forward_chars]\n\u001b[1;32m    167\u001b[0m all_forward_chars \u001b[38;5;241m=\u001b[39m pack(pad_sequence(all_forward_chars, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m--> 169\u001b[0m all_backward_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmodel_backward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_backward_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     all_backward_chars \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_backward_transform(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_backward_chars]\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:222\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    219\u001b[0m chars \u001b[38;5;241m=\u001b[39m get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id\u001b[38;5;241m=\u001b[39mvocab\u001b[38;5;241m.\u001b[39munit2id(CHARLM_END))\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 222\u001b[0m     output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[1;32m    224\u001b[0m     res \u001b[38;5;241m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yshi/lib/python3.11/site-packages/torch/nn/modules/rnn.py:920\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    917\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    918\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 920\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    923\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#orig calculate dep and const feature\n",
    "#sy\n",
    "Constituency_feature_count_map = defaultdict(int)\n",
    "Dependency_feature_count_map = defaultdict(int)\n",
    "for file in tqdm(list_txt_files(\"./LCMC_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line == '\\n':\n",
    "                continue\n",
    "            doc = nlp(line)\n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            layer_2_feature_tree = build_constituency_feature_tree(m, 2)\n",
    "            sec_feature_list = extract_constituency_feature_from_tree(layer_2_feature_tree)\n",
    "            for i in sec_feature_list:\n",
    "                Constituency_feature_count_map[i] += 1\n",
    "            \n",
    "            layer_3_feature_tree = build_constituency_feature_tree(m, 3)\n",
    "            sec_feature_list = extract_constituency_feature_from_tree(layer_3_feature_tree)\n",
    "            for i in sec_feature_list:\n",
    "                Constituency_feature_count_map[i] += 1\n",
    "           \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                Dependency_feature_count_map[dep[0]] += 1\n",
    "sorted_const_data = dict(sorted(Constituency_feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "sorted_dep_data = dict(sorted(Dependency_feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "with open(f'./orig_constituency_feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_const_data, f, ensure_ascii=False, indent=4)\n",
    "with open(f'./orig_dependency_feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_dep_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zcy add Time: 2025.01.16\n",
    "# 读取 JSON 文件\n",
    "with open('./all_feature.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#TODO: 检查data是否是字典\n",
    "\n",
    "# 遍历字典，将值改为递增的索引\n",
    "updated_data = {}\n",
    "for index, key in enumerate(data.keys()):\n",
    "    updated_data[key] = index\n",
    "\n",
    "# 写入新的 JSON 文件\n",
    "with open('./all_feature_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(updated_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index orig feature \n",
    "#sy\n",
    "with open('./all_feature.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "#TODO: 检查data是否是字典\n",
    "\n",
    "# 遍历字典，将值改为递增的索引\n",
    "updated_data = {}\n",
    "for index, key in enumerate(data.keys()):\n",
    "    updated_data[key] = index\n",
    "\n",
    "# 写入新的 JSON 文件\n",
    "with open('./all_feature_index.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(updated_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 88/88 [11:11<00:00,  7.63s/file]\n"
     ]
    }
   ],
   "source": [
    "# zcy add Time: 2025.01.16\n",
    "\n",
    "with open('./all_feature_index.json', 'r', encoding='utf-8') as f:\n",
    "    updated_data = json.load(f)\n",
    "result_vec = []\n",
    "dic_re = []\n",
    "for file in tqdm(list_txt_files(\"./instruct_txt\"), total=88, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            doc = nlp(line)\n",
    "            l1 = [0] * 629\n",
    "            dict_ft_txt = {}\n",
    "            # Constituency\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            result = build_constituency_feature_tree(m, 2)\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                if item in updated_data:\n",
    "                    l1[updated_data[item]] += 1\n",
    "                    dict_ft_txt[updated_data[item]] = item\n",
    "            result = build_constituency_feature_tree(m, 3)\n",
    "            ff = extract_constituency_feature_from_tree(result)\n",
    "            for item in ff:\n",
    "                if item in updated_data:\n",
    "                    l1[updated_data[item]] += 1\n",
    "                    dict_ft_txt[updated_data[item]] = item\n",
    "        \n",
    "            # Dependency\n",
    "            words = doc.sentences[0].words\n",
    "            dependency_list = extract_dependency_feature_from_list(words)\n",
    "            for dep in dependency_list:\n",
    "                if dep[0] in updated_data:\n",
    "                    l1[updated_data[dep[0]]] += 1\n",
    "                    dict_ft_txt[updated_data[dep[0]]] = dep[1]\n",
    "            dic_re.append(dict_ft_txt)\n",
    "            result_vec.append(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orig build vec\n",
    "\n",
    "with open('./all_feature_index.json', 'r', encoding='utf-8') as f:\n",
    "    updated_data = json.load(f)\n",
    "result_vec = []\n",
    "dic_re = []\n",
    "for file in tqdm(list_txt_files(\"./LCMC_txt\"), total=3, desc=\"Processing\", unit=\"file\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            if line == \"\":\n",
    "                doc = nlp(line)\n",
    "                l1 = [0] * 629\n",
    "                dict_ft_txt = {}\n",
    "                # Constituency\n",
    "                m = str(doc.sentences[0].constituency)\n",
    "                result = build_constituency_feature_tree(m, 2)\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    if item in updated_data:\n",
    "                        l1[updated_data[item]] += 1\n",
    "                        dict_ft_txt[updated_data[item]] = item\n",
    "                result = build_constituency_feature_tree(m, 3)\n",
    "                ff = extract_constituency_feature_from_tree(result)\n",
    "                for item in ff:\n",
    "                    if item in updated_data:\n",
    "                        l1[updated_data[item]] += 1\n",
    "                        dict_ft_txt[updated_data[item]] = item\n",
    "            \n",
    "                # Dependency\n",
    "                words = doc.sentences[0].words\n",
    "                dependency_list = extract_dependency_feature_from_list(words)\n",
    "                for dep in dependency_list:\n",
    "                    if dep[0] in updated_data:\n",
    "                        l1[updated_data[dep[0]]] += 1\n",
    "                        dict_ft_txt[updated_data[dep[0]]] = dep[1]\n",
    "                dic_re.append(dict_ft_txt)\n",
    "                result_vec.append(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zcy add Time: 2025.01.17\n",
    "with open('all_line_feature.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dic_re, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zcy add Time: 2025.01.17\n",
    "with open(\"./all_line_vec.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for row in result_vec:\n",
    "        file.write(str(row) + \"\\n\")  # 将每行写入，并添加换行符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这 是 nsubj\n",
      "并 坏事 mark\n",
      "不 坏事 advmod\n",
      "一定 坏事 advmod\n",
      "是 坏事 cop\n",
      "坏事 是 advcl\n",
      "， 坏事 punct\n",
      "尤其 是 advmod\n",
      "是 。 root\n",
      "它 发出 nsubj\n",
      "向 日本 case\n",
      "日本 发出 obl\n",
      "和 韩国 cc\n",
      "韩国 日本 conj\n",
      "这 个 det\n",
      "两 个 nummod\n",
      "个 盟友 nmod\n",
      "盟友 发出 obl\n",
      "发出 是 ccomp\n",
      "了 发出 aux\n",
      "几 个 nummod\n",
      "个 信息 clf\n",
      "至关重要 信息 amod\n",
      "的 至关重要 mark:rel\n",
      "信息 发出 obj\n",
      "。 是 punct\n"
     ]
    }
   ],
   "source": [
    "#lemma是当前token\n",
    "sent = '这并不一定是坏事，尤其是它向日本和韩国这两个盟友发出了几个至关重要的信息。'\n",
    "doc = nlp(sent)\n",
    "# print(doc.sentences[0])\n",
    "for i in doc.sentences[0].words:\n",
    "    head_id = i.head\n",
    "    print(i.lemma, doc.sentences[0].words[head_id-1].lemma, i.deprel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'upos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m                 head_word \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mwords[word\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     33\u001b[0m                 dependency_list\u001b[38;5;241m.\u001b[39mappend((word, head_word, word\u001b[38;5;241m.\u001b[39mdeprel))\n\u001b[0;32m---> 34\u001b[0m             ll\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mextract_dependency_feature_from_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdependency_list\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     36\u001b[0m         ll\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------------------------------------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 输出到文件\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m, in \u001b[0;36mextract_dependency_feature_from_list\u001b[0;34m(dependency_list)\u001b[0m\n\u001b[1;32m      6\u001b[0m     head_word \u001b[38;5;241m=\u001b[39m dep[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     relation \u001b[38;5;241m=\u001b[39m dep[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     head_upos \u001b[38;5;241m=\u001b[39m \u001b[43mhead_word\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupos\u001b[49m\n\u001b[1;32m      9\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mupos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_upos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhead_word\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mhead_word\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROOT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'upos'"
     ]
    }
   ],
   "source": [
    "# test提取依存树 暂时不用，好像错删了什么\n",
    "def extract_dependency_feature_from_list(dependency_list):\n",
    "    #lemma是当前token\n",
    "    sent = '这并不一定是坏事，尤其是它向日本和韩国这两个盟友发出了几个至关重要的信息。'\n",
    "    doc = nlp(sent)\n",
    "# print(doc.sentences[0])\n",
    "    for i in doc.sentences[0].words:\n",
    "        head_id = i.head\n",
    "        print(i.lemma, doc.sentences[0].words[head_id-1].lemma, i.deprel)\n",
    "    \n",
    "    # result = []\n",
    "    # for dep in dependency_list:\n",
    "    #     word = dep[0]\n",
    "    #     head_word = dep[1]\n",
    "    #     relation = dep[2]\n",
    "    #     head_upos = head_word.upos\n",
    "    #     result.append(f\"{word.upos}/{head_upos}/{relation} {word.text}/{head_word.text if head_word else 'ROOT'}/{relation}\")\n",
    "    # return result\n",
    "\n",
    "# 测试构造 tmp/test_feature.txt\n",
    "ll = []\n",
    "feature_sentences_map = defaultdict(list)\n",
    "with open('./instruct_txt/A01A_en_zh.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        ll.append(line.strip())\n",
    "        doc = nlp(line)\n",
    "        \n",
    "        # Constituency tree\n",
    "        if hasattr(doc.sentences[0], 'constituency'):\n",
    "            m = str(doc.sentences[0].constituency)\n",
    "            ll.append(m)\n",
    "        else:\n",
    "            ll.append(\"No constituency analysis available.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        \n",
    "        # Dependency tree\n",
    "        # for sentence in doc.sentences:\n",
    "        #     dependency_list = []\n",
    "        #     for word in sentence.words:\n",
    "        #         head_word = sentence.words[word.head - 1] if word.head > 0 else None\n",
    "        #         dependency_list.append((word, head_word, word.deprel))\n",
    "        #     ll.append(\"\\n\".join(extract_dependency_feature_from_list(dependency_list)))\n",
    "\n",
    "        ll.append(\"----------------------------------------------------------------------\")\n",
    "\n",
    "# 输出到文件\n",
    "with open('tmp/test_feature.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = dict(sorted(feature_count_map.items(), key=lambda item: item[1], reverse=True))\n",
    "with open('feature_count.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sentences_data = dict(sorted(feature_sentences_map.items(), key=lambda item: len(item[1]), reverse=True))\n",
    "with open('feature_sentences.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sorted_sentences_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'IP': {'IP': {'NP': {}, 'VP': {}}, ',': {}, 'IP83738': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}, '.': {}}}\n",
      "{'IP': {'IP': {'NP': {}, 'VP': {}}, ',': {}, 'IP': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}, '.': {}}}\n",
      "{'IP': {'NP': {}, 'VP': {}}}\n",
      "{'IP': {'LCP': {}, ',': {}, 'NP': {}, 'VP': {}}}\n"
     ]
    }
   ],
   "source": [
    "text_book = ['(ROOT (IP (IP (NP (NNP 平壤)) (VP (MD 可能) (VP (ADVP (RB 也)) (ADVP (RB 在)) (VP (VV 准备) (IP (VP (VV 试射) (NP (QP (CD 一) (CLP (NNB 枚))) (NP (NP (FW Taepo)) (FW Dong-2)) (NP (NN 导弹))))))))) (, ，) (IP (LCP (NP (NN 理论)) (IN 上)) (, ，) (NP (DP (DT 这) (CLP (NNB 种))) (NP (NN 导弹))) (VP (MD 可以) (VP (VP (VV 携带) (NP (NN 核弹) (SFN 头))) (, ，) (VP (VV 覆盖) (NP (DP (DT 整个)) (NP (NNP 美国)) (NP (NN 大陆))))))) (. 。)))']\n",
    "\n",
    "result = build_constituency_feature_tree(text_book[0], 3)\n",
    "print(result)\n",
    "\n",
    "ff = extract_constituency_feature_from_tree(result)\n",
    "for item in ff:\n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN/NUM/nummod', 'VERB/NOUN/nsubj', 'VERB/ADV/mark', 'VERB/ADV/advmod', 'VERB/VERB/advcl', 'VERB/ADV/mark', 'None/VERB/root', 'VERB/AUX/aux', 'VERB/NOUN/nmod:tmod', 'VERB/VERB/xcomp', 'NOUN/NUM/nummod', 'VERB/NOUN/obj', 'VERB/PUNCT/punct']\n"
     ]
    }
   ],
   "source": [
    "print(extract_dependency_feature_from_list(m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yshi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
